[
  {
    "title": "A Framework for Interventions in Human–AI Collaboration",
    "authors": [
      {
        "name": "On Amir",
        "affiliation": "UC San Diego"
      }
    ],
    "presenter_first_name": "Paul",
    "presenter_last_name": "Wynns",
    "presenter_email": "pwynns@ucsd.edu",
    "presenter_affiliation": "UC San Diego",
    "presentation_room": "Pfahl 240",
    "time": "10:40 - 12:00",
    "reported_discipline": "Behavioral Marketing",
    "cleaned_discipline": "Marketing",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7pRHDIVneTo0sK8",
    "abstract": "As artificial intelligence (AI) becomes embedded in high-stakes domains, public trust has emerged as a critical design challenge. Trust in AI is not just a matter of measurable performance, precision, and safety statistics. For laypeople, policymakers, and designers, perception is reality. In fields such as aviation, healthcare, and autonomous vehicles, users often assess safety based on whether human oversight appears present and engaged. Even in systems with strong safety records, discomfort can arise when human involvement is invisible or ambiguous. This paper presents a framework for trust-enhancing interventions in human–AI collaboration, grounded in theories of situational trust and social cognition. Drawing on layered trust models (Hoff & Bashir, 2015), appropriate reliance frameworks (Lee & See, 2004), and “risk-as-feelings” hypothesis (Loewenstein et al., 2001), we focus on a key perceptual driver of public trust, which is the salience and visibility of human collaboration. These models emphasize that users often evaluate AI not purely on functional criteria, but through intuitive social cues such as attentiveness, agency, and perceived control. Across four studies (N ≈ 4,500) conducted in the domain of commercial aviation, we replicate and explain the phenomenon of autopilot aversion: a measurable decline in perceived safety when automation replaces visible human control. In Study 1, we identify flight conditions such as degraded visibility that amplify discomfort with automated systems. Study 2 demonstrates that describing an otherwise identical flight as using autopilot, rather than manual control, significantly reduces perceived safety. Study 3 shows that this trust penalty is mediated by reduced perceptions of pilot attentiveness, a key signal of human engagement. Study 4 provides a proof of concep by reframing the autopilot as a collaborative partner, rather than a monitored tool, restores trust to the level observed under manual control. These findings suggest that trust in AI hinges not only on what systems do, but on how human-AI interactions are framed and understood. We propose a set of design principles grounded in social perception: make human oversight visible, frame automation as a collaborative partner, and reinforce shared attention and responsibility. Our recommendations, supported by empirical evidence, are grounded in the recognition that even so-called autonomous technologies operate within broader sociotechnical systems. In such systems, collaboration is inherent. Trust is built on how transparently and meaningfully that collaboration can be perceived. While these interventions are modest in cost and scope, they offer a way to close the perception gap in human–AI teaming. Future work should test these strategies in interactive, multimodal environments and across cultural contexts. As AI systems grow more capable, maintaining trust may depend as much on how we communicate human–AI relationships as on how we build them.",
    "session_title": "Hiring, Careers & Investment",
    "date": "Oct 2"
  },
  {
    "title": "A Human-in-the-Loop AI Agent for Meta-Analysis Coding",
    "authors": [
      {
        "name": "Hanyi Min",
        "affiliation": "University of Illinois Urbana-Champaign"
      },
      {
        "name": "Sohee Kim",
        "affiliation": "University of Illinois Urbana-Champaign"
      },
      {
        "name": "Feng Guo",
        "affiliation": "University of Tennessee at Chattanooga"
      }
    ],
    "presenter_first_name": "Hanyi",
    "presenter_last_name": "Min",
    "presenter_email": "hanyimin@illinois.edu",
    "presenter_affiliation": "University of Illinois Urbana-Champaign",
    "presentation_room": "Pfahl 140",
    "time": "9:10 - 10:30",
    "reported_discipline": "Industrial/Organizational Psychology",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_1k95hfw7mVpQLoR",
    "abstract": "Meta-analysis plays a vital role in synthesizing empirical research findings across studies, especially in psychology and the behavioral sciences. By aggregating effect sizes and study characteristics, meta-analyses enable researchers to detect patterns, identify moderators, and inform policy and theory with higher statistical power than individual studies can offer (Carter et al., 2019; Field & Gillett, 2010). However, despite its analytical strengths, conducting a meta-analysis is time-consuming and labor-intensive, primarily due to the efforts required to code study-level data. Recent advances in large language models (LLMs) and AI agents offer a transformative opportunity to address these challenges. In particular, AI agents equipped with LLMs can assist researchers in extracting structured information from unstructured texts with high levels of semantic understanding and contextual sensitivity. Yet, integrating LLMs into meta-analytic workflows requires tools that balance efficiency with interpretability, and automation with researcher control. This study introduces an AI agent system that streamlines and enhances the process of meta-analysis coding\u2014a task traditionally marked by labor-intensive, error-sensitive, and rule-based decision-making. While zero-shot large language models (LLMs) can extract text efficiently, they often fail to manage the nuanced and structured nature of meta-analytic workflows. By contrast, AI agents emulate the cognitive strategies of expert human coders through task decomposition, memory retention, iterative refinement, and adaptive reasoning (Wu et al., 2023; Yao et al., 2022). The proposed AI meta-agent is a human-in-the-loop, end-to-end system that automates the meta-analysis pipeline from full-text PDF parsing to structured data output. It integrates multiple LLMs, each optimized for specific subtasks\u2014such as document relevance screening, multi-layered information extraction, and metadata organization. Researchers can configure the agent to use different models (e.g., GPT-3.5, GPT-4.0) or parsers (e.g., LlamaParse) depending on their needs, making the system adaptable, modular, and future-proof (Mialon et al., 2023; Chen et al., 2023). The agent\u2019s workflow consists of four core steps: (1) parsing documents to extract structured text; (2) evaluating relevance based on user-specified inclusion criteria; (3) extracting multi-level information (e.g., study-level metadata, sample characteristics, variables, and effect sizes) using retrieval-augmented generation (RAG) to ensure factual grounding and minimize hallucination (Lewis et al., 2020); and (4) organizing the extracted data into standardized spreadsheets for statistical synthesis. Critically, the system retains a human-in-the-loop design, with researchers providing definitions, coding guidelines, and clarifications to resolve ambiguities. This hybrid approach enables high-throughput data processing while preserving the contextual expertise and methodological rigor essential for high-quality meta-analyses. As a result, the AI meta agent significantly reduces time and effort while improving accuracy, transparency, and scalability.",
    "session_title": "Human-in-the-Loop AI",
    "date": "Oct 2"
  },
  {
    "title": "Accuracy Obsession: Humans Prioritize Immaterial AI Accuracy Over Their Own Compensation \u2014 Unless We Educate Them",
    "authors": [
      {
        "name": "Matthew DosSantos DiSorbo",
        "affiliation": "Harvard Business School"
      },
      {
        "name": "Kris Johnson Ferreira",
        "affiliation": "Harvard Business School"
      }
    ],
    "presenter_first_name": "Matthew",
    "presenter_last_name": "DosSantos DiSorbo",
    "presenter_email": "mdisorbo@hbs.edu",
    "presenter_affiliation": "Harvard Business School",
    "presentation_room": "Pfahl 202",
    "time": "3:00 - 4:40",
    "reported_discipline": "Technology and Operations Management",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3wAEXCqTC8szjs8",
    "abstract": "1. Problem definition: Accuracy isn't everything. In many contexts, high or low sensitivity is more important than overall correctness \u2014 and yet, we find that humans prefer accurate algorithms, even to their detriment. Specifically, we identify and address an accuracy obsession: humans trust algorithms based on the algorithm's accuracy, not the actual monetary reward they could earn by using the algorithm. 2. Methodology/results: We model a two-stage algorithm that first estimates the probability of a positive case and, if that probability is above (below) a pre-determined threshold, then recommends (does not recommend) a human review; the human then makes the final decision to review, or not. Penalties and rewards incurred by the human vary across outcomes and settings: for example, the cost of a false negative might be substantially larger than the cost of a false positive. However, we introduce theory that suggests humans have an accuracy obsession: trust in the algorithm\u2019s recommendations is solely based on algorithm accuracy (how often the algorithm\u2019s recommendation matches the true outcome), not performance (the reward earned by following the algorithm\u2019s recommendations). A set of online lab experiments confirms our hypotheses: participants trusted a sub-optimal algorithm calibrated only to maximize accuracy, not reward, much more than an optimal algorithm calibrated to maximize reward. These results held across two contrasting settings: high and low penalty for false negatives and, in turn, an optimal algorithm that is much more,and much less, sensitive than the sub-optimal algorithm. Finally, we design an educational intervention that describes how the optimal algorithm works \u2014 i.e., its recommendation threshold is determined by the penalties and rewards of different outcomes, not overall accuracy \u2014 and provides transparency into the algorithm\u2019s probability estimates. This intervention successfully mitigates the accuracy obsession in both settings. 3. Managerial Implications: We demonstrate that humans are biased towards an accuracy obsession: trusting an algorithm based on its accuracy, not its expected reward. We introduce a simple educational intervention that mitigates this bias, and hope that our work will help managers best prepare their employees for success in the many human-AI collaborations where accuracy isn't everything.",
    "session_title": "AI in Markets, Consumers & Branding",
    "date": "Oct 2"
  },
  {
    "title": "Addressing Consumers\u2019 Sensitive Attributes in Product Recommendations: An Explainable AI Recommendations System Approach",
    "authors": [
      {
        "name": "Piyush Anand",
        "affiliation": "Rice University"
      }
    ],
    "presenter_first_name": "Piyush",
    "presenter_last_name": "Anand",
    "presenter_email": "piyush.anand@rice.edu",
    "presenter_affiliation": "Rice University",
    "presentation_room": "Pfahl 202",
    "time": "3:00 - 4:40",
    "reported_discipline": "Marketing",
    "cleaned_discipline": "Marketing",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3tlHWjKmiuv8pFz",
    "abstract": "Sensitive consumer attributes such as race are an important concern for marketing managers for their recommendation systems. Regulatory guidelines for sensitive attributes vary across consumer contexts. Industries such as consumer finance and technology have strict regulations prohibiting usage of race, whereas other industries such as cosmetics lack such guidelines. Merely ignoring sen- sitive attributes can be suboptimal when recommendations are correlated with sensitive attributes. This work advances the Xian et al. (2019) knowledge graph and policy-guided path reasoning approach with a regularization framework to address sensitive attributes in product recommen- dations. We demonstrate the efficacy of the proposed approach on products in the cellphones category using customer reviews data from Amazon. The proposed approach provides near-highest recommendation quality and outperforms benchmarks on decorrelating sensitive attributes with recommendations. The proposed approach is also flexible in handling consumer attributes, as an extension we examine the beauty category where it leverages race and outperforms benchmarks on recommendation quality. Explaining reasons for product recommendations is straightforward with the proposed approach because of the path-reasoning method. Marketing managers will find the proposed approach useful as regulators and consumers increasingly demand explainability for firms\u2019 recommendations.",
    "session_title": "AI in Markets, Consumers & Branding",
    "date": "Oct 2"
  },
  {
    "title": "AI and the Extended Workday: Productivity, Contracting Efficiency, and Distribution of Rents",
    "authors": [
      {
        "name": "Junyoung Park",
        "affiliation": "Auburn University"
      },
      {
        "name": "Wei Jiang",
        "affiliation": "Emory University"
      },
      {
        "name": "Rachel Xiao",
        "affiliation": "Fordham University"
      },
      {
        "name": "Shen Zhang",
        "affiliation": "Fordham University"
      }
    ],
    "presenter_first_name": "Junyoung",
    "presenter_last_name": "Park",
    "presenter_email": "jzp0170@auburn.edu",
    "presenter_affiliation": "Auburn University",
    "presentation_room": "Pfahl 202",
    "time": "1:00 - 2:40",
    "reported_discipline": "Information Systems",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_52EP1t9EsKCZvXj",
    "abstract": "While much of the discussion on artificial intelligence and the labor market has focused on job displacement and creation (e.g., Babina et al., 2024; Hampole et al., 2025; Xue et al., 2022; Zhang et al., 2024), less attention has been paid to how AI affects the intensive margin of work\u2014work hours, contracting efficiency, and the distribution of productivity gains. This paper investigates the micro-level impacts of AI on individuals\u2019 time allocation and its broader organizational and economic implications. The theoretical relationship between AI and work hours is ambiguous. AI may shorten task duration and raise income, enabling more leisure. However, AI-enhanced productivity and improved performance monitoring can lead to longer work hours, especially when workers lack bargaining power (Holmstrom & Milgrom, 1987). We explore these dynamics using data from the American Time Use Survey (ATUS), covering approximately 26,400 individuals annually from 2004 to 2023. We utilized OpenAI\u2019s large language model to measure occupational AI exposure through the average textual similarity between work task descriptions and AI-related patents and occupational AI complementarity through the average indicators of whether a patent complements or substitutes a work task. Additionally, we measure occupational generative AI (GenAI) exposure through indicators of whether a work task is exposed to GenAI technologies. We find that the average occupational AI exposure increases over time (Figure 1), and workers in higher-AI-exposed occupations report longer work hours, especially in recent years (Figure 2). To identify causal effects, we use the advent of ChatGPT in late 2022 as a natural experiment. We find that workers in higher occupational GenAI exposure experienced a significant increase in work hours and a reduction in leisure time, especially in occupations complementary to GenAI and regions with greater AI awareness (Table 1). These patterns also hold across the full sample period when examining general AI exposure (Table 2). Notably, reductions in leisure hours are concentrated in non-screen-based activities like traveling. To interpret these findings, we test predictions from the Principal-Agent model along three dimensions: (1) Marginal productivity: Work hours and wages both rise with AI exposure (Table 3). (2) Monitoring efficiency: Remote workers with higher AI-based surveillance work longer hours (Table 4). (3) Reservation utility: The extension of work hours is more pronounced when labor and product market competition is intense, suggesting workers are unable to fully internalize the productivity gains (Table 5). This study contributes to the literature on organizational change and the future of work in the AI era by highlighting how AI transforms the structure and intensity of work within existing jobs. Our findings suggest that in the AI era, firms are reorganizing how effort is monitored, contracts are enforced, and productivity gains are distributed, leading to longer workdays. This challenges the common expectation that technological progress leads to more leisure, revealing new tensions between efficiency and well-being in modern work arrangements.",
    "session_title": "AI, Work & Organizations",
    "date": "Oct 2"
  },
  {
    "title": "AI in the Pen: How Real-Time AI Writing Guidance Shapes Online Reviews",
    "authors": [
      {
        "name": "Zaiyan Wei",
        "affiliation": "Purdue University"
      },
      {
        "name": "Fangyan Wang",
        "affiliation": "Purdue University"
      },
      {
        "name": "Sai Liang",
        "affiliation": "Nankai University"
      }
    ],
    "presenter_first_name": "Zaiyan",
    "presenter_last_name": "Wei",
    "presenter_email": "zaiyan@purdue.edu",
    "presenter_affiliation": "Purdue University",
    "presentation_room": "Pfahl 140",
    "time": "11:10 - 12:10",
    "reported_discipline": "Management Information Systems",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_5q82kINHtyZzxOS",
    "abstract": "Artificial intelligence (AI) support for content creation is proliferating, yet most studies focus on AI systems that generate content outright, leaving open questions about tools that guide users as they write. We address this gap by studying how real-time AI writing guidance affects online reviews, an important form of user-generated content (UGC) that has been shown to influence consumer decision-making and business reputation. Specifically, we examine Yelp\u2019s rollout of an AI writing guidance feature in April 2023. Drawing on the Self-Determination Theory, we devise differences-in-differences (DID) approaches to evaluate the impact of the AI writing guidance on review outcomes. The feature displays a checklist of keywords, i.e., Food, Service, and Ambiance, and highlights each topic once the review writer addresses it, thereby scaffolding the writing process without composing any text itself. We obtain more than 1.5 million Yelp reviews from seven major cities in the United States and construct weekly panels for the years 2022 (unaffected) and 2023 (affected) in our main DID design. To provide a comprehensive understanding of the impact, we examine not only the quantities and textual features of reviews (e.g., review length, review volume, and natural language processing measures of topic coverage, readability, and novelty that are facilitated by generative AI tools) but also review quality indicators (helpfulness votes, argument quality, etc.). Our analysis yields several key findings. First, Yelp\u2019s AI writing guidance significantly increased the textual length of reviews while reducing the total volume (number of reviews generated), suggesting an interesting trade-off between depth and productivity. However, such a trade-off between length and volume was less prominent among reviews of less popular restaurants. Second, reviews generated under the AI writing guidance increasingly emphasized AI-suggested topics, particularly service and ambiance\u2014dimensions that are often underrepresented relative to food. Third, despite a slight improvement in the argument quality of reviews, we find that the AI writing guidance led to a decrease in perceived helpfulness from the audience, which can be partially attributed to greater complexity, reduced readability, and less novelty of reviews under the AI writing guidance. Lastly, although reviews written by both experienced and inexperienced users exhibited similar changes in textual features, those written by experienced users received even fewer helpfulness votes, reflecting a greater penalty in perceived review quality. We contribute to the literature on human-AI interactions by shifting the focus to examining AI systems that guide users as they write (rather than generating the content for them). The findings demonstrate that AI scaffolding can simultaneously enrich content breadth and erode perceived value, thus highlighting a tension between the competence it affords content creators and the autonomy it constrains. Managerially, the study suggests that finer-grained or personalized guidance, such as showing checklists only to novice contributors, may capture the informational benefits of AI assistance without dampening participation or reader utility.",
    "session_title": "AI & Responses",
    "date": "Oct 3"
  },
  {
    "title": "AI-Powered Virtual Stress-Testing: A Data-Driven Framework for Strategic Resilience Investment in Power Grids",
    "authors": [
      {
        "name": "Mazin Al-Mahrouqi",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Abdollah Shafieezadeh",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Jieun Hur",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Mazin",
    "presenter_last_name": "Al-Mahrouqi",
    "presenter_email": "mahrouqi.1@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 240",
    "time": "3:00 - 4:40",
    "reported_discipline": "Civil Engineering",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7aOKcz7cWssGmEp",
    "abstract": "The urgent need for new transmission infrastructure, driven by the expansion of data centers and the AI boom, intensifies a long-standing challenge for utility providers: accurately assessing the fragility of transmission lines against extreme weather events. Current physics-based simulations, while foundational, are often conservative, computationally intensive, and struggle to capture the complex interplay of real-world variables like elevation, environmental conditions, and multifaceted weather patterns. This analytical gap leads to suboptimal capital allocation for grid hardening, leaving customers and economies vulnerable to extended, high-cost power outages. Fortunately, decades of grid operation have produced a vast repository of historical interruption records. This real-world data inherently captures the complex, system-wide effects that simulations often oversimplify. We propose to utilize this rich dataset with a novel, data-driven AI model that functions as an event-based 'virtual stress-test.' Developed in collaboration with the Korea Atomic Energy Research Institute (KAERI), our approach is distinguished by its foundation in real-world data. It is trained on an extensive dataset of actual, historical power grid interruptions from the Bonneville Power Administration, spanning 22 years and covering a wide range of weather patterns and terrains. This grounding in empirical evidence ensures its relevance to operational challenges, setting it apart from models based on simulated or testbed systems. The model takes a given weather event as input to predict and rank which specific lines are most likely to fail. This framework also enables 'virtual hardening testing,' allowing planners to simulate how strategic upgrades—such as adjusting a line's voltage—would alter its failure probability under the same stress event, thereby identifying the most effective resilience strategies. To translate these technical fragility assessments into actionable business intelligence, we propose a methodology for integrating the model's outputs into a Cost-Benefit Analysis (CBA) framework. By coupling the model's predicted failure probabilities with established economic metrics, such as the Value of Lost Load (VoLL), this approach is designed to quantify the financial benefit of a proposed investment in terms of avoided societal and operational costs. This provides a transparent, data-driven basis for justifying capital expenditures to regulators and stakeholders, ensuring that investments are directed toward the most impactful resilience improvements. This research directly aligns with the conference theme of AI tools for strategic decision-making. Our framework empowers utility planners to move beyond generalized risk assessments toward a proactive, financially optimized approach to resilience. It serves as a Human-in-the-Loop system where the AI provides a quantitative risk profile, augmenting the strategic capabilities of human decision-makers to test hypotheses, prioritize investments, and build a more reliable and resilient energy future.",
    "session_title": "AI & Responses",
    "date": "Oct 2"
  },
  {
    "title": "AI Self-preferencing in Algorithmic Hiring: Empirical Evidence and Insights",
    "authors": [
      {
        "name": "Jiannan Xu",
        "affiliation": "University of Maryland"
      },
      {
        "name": "Gujie Li",
        "affiliation": "National University of Singapore"
      },
      {
        "name": "Jane Yi Jiang",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Jiannan",
    "presenter_last_name": "Xu",
    "presenter_email": "jiannan@umd.edu",
    "presenter_affiliation": "University of Maryland",
    "presentation_room": "Pfahl 140",
    "time": "1:00 - 2:40",
    "reported_discipline": "Operations Management",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": true,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6QXL7OHOrPDpehf",
    "abstract": "Generative Artificial Intelligence (AI) is transforming the hiring landscape, with job applicants increas- ingly using Large Language Models (LLMs) to craft resumes and employers integrating LLMs into their recruitment pipelines. This dual adoption of AI raises urgent ethical concerns, particularly around AI self- preferencing bias \u2014 the tendency of an algorithm to favor its own generated content over content written by humans or generated by other models. This study empirically investigates the presence and extent of AI self-preference bias in algorithmic hiring. We distinguish between two forms of bias: (i) an LLM preferring its own generated resume over human-written ones, and (ii) an LLM preferring its own generated resume over those generated by alternative LLMs. Using a series of controlled resume correspondence experiments, we find strong and systematic evidence of both types of self-preferencing across widely used commercial and open-source LLMs. The bias against human-written resumes is more substantial and pervasive, with self-preference bias ranging from 68% to 92% across major models. These biases persist even after control- ling for content quality through human annotations. Our findings reveal significant risks for fairness in the labor market, particularly for candidates who do not have access to AI tools. We conclude with policy and design recommendations to mitigate AI self-preferencing and promote more equitable and transparent hiring practices.",
    "session_title": "Best Presentation Competition \u2013 Doctoral Students",
    "date": "Oct 2"
  },
  {
    "title": "AI-Assisted Triage of Patient Messages: A Clinically Calibrated Human\u2013AI Framework for Healthcare Operations",
    "authors": [
      {
        "name": "Mohamed Megahed",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Nathan Craig",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Aravind Chandrasekaran",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Mohamed",
    "presenter_last_name": "Megahed",
    "presenter_email": "megahed.4@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 202",
    "time": "9:10 - 10:30",
    "reported_discipline": "Operations Management",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3eWYnOWvuaGSGhw",
    "abstract": "This study addresses clinician burnout from increased asynchronous patient messaging by developing and evaluating an AI-assisted triage system. Leveraging natural language processing and clinician-calibrated severity scoring, the system prioritizes message s for faster, more effective responses. Using data from 3.2 million messages, the research compares four triage workflows, linking severity flags to adverse clinical events. Results show AI-human collaboration improves response time and outcomes while main taining alignment with clinician judgment. This integrated framework offers a scalable approach to enhancing healthcare operations and advancing human-AI interaction theory.",
    "session_title": "AI in Healthcare & Learning",
    "date": "Oct 2"
  },
  {
    "title": "Algorithm Aversion in Supplier Selection - The Role of Cognitive Style and Interface Design",
    "authors": [
      {
        "name": "Abhinav Hasija",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Aravind Chandrasekaran",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Terry Esper",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Thomas Goldsby",
        "affiliation": "University of Tennessee"
      },
      {
        "name": "Walter Zinn",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Abhinav",
    "presenter_last_name": "Hasija",
    "presenter_email": "hasija.4@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 140",
    "time": "1:00 - 2:40",
    "reported_discipline": "Logistics",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": true,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_5n081H8xBCHvkeB",
    "abstract": "Algorithmic decision-support systems (ADSS) promise to streamline complex supply-chain decisions, yet managers still override their recommendations with surprising frequency. To unpack this resistance, we examine its cognitive roots and test two interface designs intended to keep humans productively in the loop. Our empirical context is carrier selection in spot-market transportation, a canonical supplier-selection task that forces decision makers to balance cost, service, and risk under tight time pressure. We conducted two preregistered online experiments that yielded 477 usable observations. Study 1 manipulated task complexity by presenting either a simple two-attribute choice set or a demanding seven-attribute scenario. Participants\u2019 propensity to reject the ADSS was modelled with logistic regression as a joint function of complexity and Cognitive Reflection Test (CRT) score, a well-established proxy for analytical thinking style. A strong interaction emerged: when the task was simple, high-CRT (reflective) thinkers were markedly more likely to accept the system\u2019s recommendation than low-CRT (intuitive) thinkers (predicted acceptance rates of 72% versus 49%). When the task was complex, the pattern reversed (38% versus 56%). The finding suggests that aversion is not a fixed trait; instead, it is a context-sensitive reaction to perceived competence gaps between human and machine. Study 2 held complexity high and independently introduced two empowerment levers that already appear in commercial dashboards. Input control allowed users to adjust attribute weights before the algorithm ran, preserving a sense of authorship. Local explainability pro- vided a concise, sentence-length post-hoc rationale that highlighted the three most influential attributes behind each recommendation. Controlling for domain experience, numeracy, and trust propensities, the explanatory sentence reduced the odds of rejection by roughly 50% (odds ratio = 0.47, p < . 001) and quadrupled behavioural compliance with the algorithm\u2019s choice. Input control showed no statistically significant effect on either aversion or compli- ance ( p=.41). Supplementary analyses confirmed that explanations offered more than mere appeasement: they improved objective performance, lowering ex-post regret (the cost differ- ence between the chosen and optimal carrier) by 18% on average. A robustness check using hierarchical models clustered at the participant level produced qualitatively identical results. Together, the experiments advance theory in three ways. First, they demonstrate that algo- rithm aversion in supply-chain contexts is malleable and jointly shaped by cognitive style and task design. Second, they disentangle two often conflated empowerment mechanisms, show- ing that transparent reasoning\u2014not superficial interactivity\u2014builds trust. Third, they provide rare causal evidence that targeted transparency can align human and machine decisions without requiring extensive user training. For practitioners, the results yield a clear prescription. Adding a brief causal explanation to ADSS outputs imposes negligible implementation cost yet delivers a sizable trust dividend, particularly among reflective employees who are most inclined to scrutinise complex models. By contrast, additional sliders and dials may furnish only an illusion of control, leaving both confidence and performance unchanged. By integrating individual cognition, system transparency, and measurable performance in a realistic supplier-selection setting, the study informs the broader agenda on responsible human-AI collaboration and offers actionable guidance for deploying ADSS in strategic logistics decisions.",
    "session_title": "Best Presentation Competition \u2013 Doctoral Students",
    "date": "Oct 2"
  },
  {
    "title": "Amplifying Human Capacity: Institutionalizing Human-in-the-Loop AI at a Land Grant University",
    "authors": [
      {
        "name": "Mark Stone",
        "affiliation": "University of Nebraska-Lincoln/Agricultural Economics"
      }
    ],
    "presenter_first_name": "Mark",
    "presenter_last_name": "Stone",
    "presenter_email": "mark.stone@unl.edu",
    "presenter_affiliation": "University of Nebraska-Lincoln/Agricultural Economics",
    "presentation_room": "Pfahl 140",
    "time": "9:10 - 10:30",
    "reported_discipline": "Biological Systems Engineering",
    "cleaned_discipline": "Pedagogy",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7psrBTjXm3u7Lmt",
    "abstract": "As artificial intelligence (AI) becomes pervasive in organizational decision-making, the need to harness its potential while safeguarding human agency, ethical responsibility, and sustainable innovation grows increasingly urgent. This presentation details the University of Nebraska\u2013Lincoln\u2019s (UNL) Pioneering Regenerative AI for Innovation, Research, and Inclusive Education (PRAIRIE) initiative is a comprehensive, human-centered approach that positions AI not as a mere automation tool but as a partner for capacity-building and community empowerment. By aligning operational, educational, and research practices with Nebraska\u2019s land-grant mission, PRAIRIE exemplifies how institutions can responsibly implement human-in-the-loop AI across diverse business and organizational contexts. UNL PRAIRIE\u2019s framework is built upon four core pillars: empowering AI-ready learners, strengthening institutional and industry partnerships, catalyzing discovery through targeted research, and building sustainable, AI-enhanced systems. Distinctive in its focus on \u201cRegenerative AI,\u201d the initiative emphasizes systems that augment human capability and foster cycles of sustainable innovation, rooting AI deployment in stewardship, inclusion, and pragmatic co-creation. \u25cf Empowering AI-Ready Learners: By 2027, every UNL engineering graduate will be fluent in both their technical discipline and AI applications relevant to their field. PRAIRIE integrates AI modules throughout curricula, offers new certificate programs (\u201cAI x Engineering\u201d and \u201cAI x All\u201d), and provides institution-wide AI literacy development for staff and faculty. AI tools are deployed to personalize learning and broaden access, ensuring every learner, regardless of background, is positioned for success in AI-enabled workplaces. \u25cf Strengthening Partnerships: PRAIRIE serves as a bridge between innovation and local needs, particularly across Nebraska\u2019s manufacturing, infrastructure, agricultural, and rural sectors. The initiative partners with industry to offer microcredential programs for professionals, hosts K-12 teacher and student academies, and accelerates the diffusion of responsible AI practices outside the university. \u25cf Driving Discovery: An AI Applications Core and strategic seed funding support interdisciplinary research in infrastructure, rural innovation, and biotechnology. By recruiting new faculty and building external partnerships, PRAIRIE aims to secure major federal funding while positioning Nebraska as a heartland leader in responsible AI application and research. \u25cf Building Sustainable Systems: AI-enhanced platforms improve student retention, operational efficiency, and policy governance. Crucially, governance structures protect against misuse, support ethical decision-making, and institutionalize mechanisms for continuous stakeholder feedback. Drawing on empirical findings from PRAIRIE\u2019s initial deployment, we share evidence-based strategies for overcoming psychological and organizational barriers to adoption, fostering communities of practice, and differentiating roles in early AI experimentation. Our research shows that human-centered onboarding, transparent governance, and participatory program design are key to reducing anxiety and amplifying motive power for innovation. This session provides both conceptual models and practical tools such as AI-readiness assessments, modular training workshops, and adaptable policy templates, which are relevant to business, government, and educational leaders. By centering human capacity, PRAIRIE offers a replicable roadmap for organizations seeking to embed human-in-the-loop AI: not merely for technical efficiency, but to enhance collaboration, inclusion, and resilience in an AI-transformed world.",
    "session_title": "Human-in-the-Loop AI",
    "date": "Oct 2"
  },
  {
    "title": "Artificial Intelligence, Opportunity, and Regulatory Uncertainty: Implications for Asset Pricing",
    "authors": [
      {
        "name": "Kris Shen",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Kris",
    "presenter_last_name": "Shen",
    "presenter_email": "shen.1732@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 240",
    "time": "1:00 - 2:40",
    "reported_discipline": "Finance",
    "cleaned_discipline": "Finance",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7RQ7GWfZMCOWHtr",
    "abstract": "This paper studies two channels\u2014opportunity and regulatory uncertainty\u2014through which Artificial Intelligence (AI) affects the stock prices and risk premia. On the one hand, advances in AI present firms with opportunities, leading them to exhibit characteristics of growth firms and earn lower expected returns. On the other hand, firms face increased regulatory uncertainty in AI development, increasing their political risk exposure and resulting in higher expected returns. Using conference call transcripts, I construct a firm-level measure of AI Exposure that captures the level of attention analysts and managers devote to AI-related topics at specific points in time. Empirically and theoretically, I show that these two channels exert opposing effects: firms focused on opportunity earn a negative AI risk premium, while those more affected by regulatory uncertainty earn a positive AI risk premium.",
    "session_title": "AI in Finance & Accounting",
    "date": "Oct 2"
  },
  {
    "title": "Auditing in the Age of AI: Private Equity vs. the Public Interest",
    "authors": [
      {
        "name": "Rajib Doogar",
        "affiliation": "University of Washington Bothell"
      },
      {
        "name": "Sri Ramamoorti",
        "affiliation": "University of Dayton"
      },
      {
        "name": "Cory Campbell",
        "affiliation": "Indiana State University"
      }
    ],
    "presenter_first_name": "Rajib",
    "presenter_last_name": "Doogar",
    "presenter_email": "doogar@uw.edu",
    "presenter_affiliation": "University of Washington Bothell",
    "presentation_room": "Pfahl 240",
    "time": "1:00 - 2:40",
    "reported_discipline": "Accounting",
    "cleaned_discipline": "Accounting",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3fiZRcl6BlNrpdQ",
    "abstract": "Accounting and auditing are being revolutionized by the advent of Agentic AI (AAI), positioning the public accounting profession at a delicate inflection point. Properly conceived deployment of AAI that respects the primacy of the human in the loop (HITL) in the audit process promises to expand the scope and value of financial reports and assurance services beyond levels currently dreamt of, ushering in a \u201cgolden age\u201d of auditing. AAI adoption requires sizeable and risky upfront investments. The widespread use of PE financing by accounting firms to fund AAI adoption, however, reduces audit partners\u2019 \u201cskin in the game,\u201d impairs auditor independence and credibility and diminishes perceived audit quality. The stark and consequential tradeoff between the promise of AI and the perils inherent in its manner of financing poses an existential risk for accounting firms and a challenge for regulators tasked with protecting the public interest. On the supply side, AAI is already being deployed to reduce audit costs by eliminating much of the evidence gathering and documentation effort that adds little value relative to the more-critical audit opinion formation process. On the demand side, AAI can enable auditors to provide assurance about entity-level impacts of increasingly volatile, uncertain, complex and ambiguous (VUCA) environmental factors that will appeal to a broad array of stakeholders. And at a structural level, AAI will also profoundly alter the management of human capital \u2013 one of the most critical tasks a professional services organization faces - both at the level of the profession as a whole as well as within audit firms. The disruptive impact of AAI is already evident in the dismantling of the traditional audit firm pyramid. Clearly, the auditor of the future will need vastly different education and training to successfully navigate the landscape of professional practice. Managing this transition will require significant adaptations by both the profession and academy. As noted earlier, deploying AAI in the right way requires investments on a scale that small and most mid-sized CPA firms cannot even contemplate. The use of PE financing to fund these investments, however, reduces the economic bond posted by the auditor (i.e., \u201cskin in the game\u201d), impairs auditor independence and reduces auditor incentives to abide by professional ethical obligations. PE financing of AAI adoption may thus have the Pyrrhic consequence of undermining the value of the very services that it makes possible!",
    "session_title": "AI in Finance & Accounting",
    "date": "Oct 2"
  },
  {
    "title": "Backpropagating from Customer Value",
    "authors": [
      {
        "name": "Midam Kim",
        "affiliation": "ServiceNow"
      },
      {
        "name": "Fabio Casati",
        "affiliation": "ServiceNow"
      },
      {
        "name": "Darrell Penta",
        "affiliation": "ServiceNow"
      },
      {
        "name": "Delaney Kipple",
        "affiliation": "ServiceNow"
      },
      {
        "name": "Catherine Martin",
        "affiliation": "ServiceNow"
      },
      {
        "name": "Minyoung Kim",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Di Yan",
        "affiliation": "TU Delft"
      }
    ],
    "presenter_first_name": "Midam",
    "presenter_last_name": "Kim",
    "presenter_email": "midamkim@gmail.com",
    "presenter_affiliation": "ServiceNow",
    "presentation_room": "Pfahl 202",
    "time": "3:00 - 4:40",
    "reported_discipline": "Management",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "midamkim",
    "abstract": "As agentic AI grows in complexity and scale, adapting to human-level variability and expanding through widespread adoption, evaluating its performance poses increasing challenges. How can enterprises tackle this evaluation complexity to maximize its value creation? This study examines how cross-functional collaboration shapes AI evaluation within an enterprise and explores how it can be optimized for greater value creation. Grounded in the value chain theory and the problem-solving view of the firm, we develop a framework that links evaluation complexity, organizational collaboration degree, and value creation. We hypothesize that while higher evaluation complexity may hinder value creation for customers, stronger cross-functional collaboration can mitigate that negative effect of evaluation complexity. We conduct a mixed-method approach, combining qualitative data from semi-structured interviews with stakeholders to model the AI value chain and quantitative research through organizational network analysis and hypothesis testing. This work offers a systems-level view on collaboration practices for AI evaluation in an enterprise and investigates whether tighter collaboration across cross-functional teams in the AI value chain can drive better outcomes for value creation.",
    "session_title": "AI in Markets, Consumers & Branding",
    "date": "Oct 2"
  },
  {
    "title": "Between Awe and Shame: The Emotional and Interpersonal Consequences of Workplace AI Use",
    "authors": [
      {
        "name": "Justin Woodall",
        "affiliation": "University of Georgia"
      },
      {
        "name": "Yihao Liu",
        "affiliation": "University of Georgia"
      },
      {
        "name": "Szu-Han (Joanna) Lin",
        "affiliation": "University of Georgia"
      },
      {
        "name": "Jack Ting-Ju Chiang",
        "affiliation": "Peking University"
      },
      {
        "name": "Zheng Wang",
        "affiliation": "Zhejiang University"
      }
    ],
    "presenter_first_name": "Justin",
    "presenter_last_name": "Woodall",
    "presenter_email": "justin.woodall@uga.edu",
    "presenter_affiliation": "University of Georgia",
    "presentation_room": "Pfahl 202",
    "time": "10:40 - 12:00",
    "reported_discipline": "Management - Organizational Behavior",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_61F1fcQxMk1Bmbo",
    "abstract": "Many employees who use artificial intelligence (AI) at work find themselves caught in an emotional tug-of-war: while they are eager to harness AI's potential to boost their performance, they simultaneously worry about how their AI use may be negatively perceived by colleagues. In this research, we depart from the emerging literature's primarily cognitive and performance focus to instead take an emotional and interpersonal perspective on workplace AI use. Drawing on Lazarus' appraisal theory of emotions, we propose that workplace AI use simultaneously triggers both awe and shame in employees. AI's extraordinary efficiency-enhancing capabilities elicit feelings of awe, motivating employees to reveal their AI use to coworkers. In contrast, concerns about appearing incompetent, lazy, or unethical evoke feelings of shame, driving employees to conceal their reliance on this technology from coworkers. We further propose that revealing AI use enhances employees' social integration at work, whereas concealing it undermines their workplace relationships. To test these propositions, we employed a multi-method approach combining a preregistered experiment (N = 190) and a multi-wave, multi-source field study with 676 employees across 112 work teams. Both studies supported our core predictions: AI use simultaneously evokes awe and shame, which drive employees' decisions to reveal or conceal their AI usage, respectively. Our field study further demonstrated that while revealing AI use increased only self-rated social integration, concealing AI use decreased both self-rated and coworker-rated social integration, underscoring the interpersonal costs of concealment. Building on appraisal theory's emphasis on context in shaping emotional experiences, we also examined whether task routineness moderates these effects. As predicted, using AI for routine tasks strengthened the indirect effect of AI use on revealing via awe and weakened the indirect effect of AI use on concealing via shame. These findings advance our understanding of AI in organizations in three important ways. First, we shift the focus from what employees think about AI to how they feel when using it, showing that workplace AI adoption is not merely a rational decision but a deeply emotional experience that shapes social behavior. While prior research has examined cognitive evaluations of AI's usefulness and fairness, our work demonstrates that emotions play a central role in how employees navigate AI integration. Second, we uncover the hidden social dynamics of AI use by showing how employees actively manage the reputational risks of this technology through strategic disclosure decisions. Importantly, we find that concealment backfires: while hiding AI use aims to protect one's professional image, it actually harms workplace relationships, whereas transparency benefits employees' own sense of social connection. Third, we contribute to theory on AI adoption by demonstrating how structural aspects of work tasks shape employees' feelings about AI use. While prior research has focused primarily on individual differences, our study reveals how the nature of work tasks\u2014specifically their routineness\u2014fundamentally shapes employees' emotions and disclosure decisions.",
    "session_title": "Trust, Emotion & Human Experience",
    "date": "Oct 2"
  },
  {
    "title": "Breaking the Sound Barrier: Asymmetric Impacts of AI Dubbing on Multilingual Engagement on YouTube",
    "authors": [
      {
        "name": "Minjie Han",
        "affiliation": "University of Rochester"
      },
      {
        "name": "Mikhail Lysyakov",
        "affiliation": "University of Rochester"
      },
      {
        "name": "Yang Gao",
        "affiliation": "University of Illinois Urbana-Champaign"
      }
    ],
    "presenter_first_name": "Minjie",
    "presenter_last_name": "Han",
    "presenter_email": "mhan14@simon.rochester.edu",
    "presenter_affiliation": "University of Rochester",
    "presentation_room": "Pfahl 140",
    "time": "1:00 - 2:40",
    "reported_discipline": "Information Systems",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": true,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7Igw0ugegkp0Hwi",
    "abstract": "The global video content market faces significant language barriers that hinder audience engagement. YouTube, with over 2.49 billion monthly active users, predominantly features English-language content, despite only about 18% of the global population being English speakers. To address this disparity, YouTube introduced AI-generated dubbing in December 2024, automatically translating and synthesizing speech in multiple languages. English videos were dubbed in 8 other languages (French, German, Hindi, Indonesian, Italian, Japanese, Portuguese, and Spanish), while videos in those 8 other languages were dubbed in English. Our paper aims to answer two primary research questions: How does AI dubbing impact user engagement? Are the impacts of AI dubbing heterogeneous across different source languages? Media Richness Theory (Daft and Lengel, 1986) and Media Naturalness Theory (Kock, 2004) posit that content with linguistic features resembling natural human communication enhances engagement by reducing cognitive effort and increasing processing fluency. We argue that AI-dubbed videos, by enabling native-language audio experiences, offer a richer and more cognitively accessible alternative to subtitles, thereby attracting new international audiences who speak the dubbed languages. Complementing this view, Cultural Proximity Theory (Straubhaar, 1991) and the concept of Cultural Discount (Hoskins and Mirus, 1988) suggest that audiences are more likely to engage with content that is culturally and linguistically familiar. Given the historically asymmetric global influence of Western media, we expect English-language channels to benefit more from AI dubbing, as it reduces linguistic barriers for international audiences interested in mass-appealing English content. In contrast, non-English channels may face intensified competition and higher cultural discount, attenuating the effectiveness of AI dubbing in attracting cross-cultural viewers. Analyzing data from a random sample of 5,000 information and knowledge-related channels between October 15, 2024, and January 20, 2025, we employ Propensity Score Matching (PSM) and Difference-in-Differences (DID) methods. We find that AI dubbing significantly increases engagement by 5.4-7.8%, particularly comments from viewers whose language differs from the source channel. However, this effect is asymmetric: English-language channels benefit more substantially (6.7% to 10% lift in engagement), while non-English channels do not experience significant gains when dubbed in English. Our results remain valid in robustness checks. A mechanism analysis reveals that a channel's content complexity positively moderates the effect of AI dubbing on engagement, suggesting reduced cognitive load for more complex material. In contrast, the emotional expressiveness of content creators\u2014measured through valence and arousal scores in audio samples\u2014negatively moderates the impact, highlighting current AI limitations in conveying nuanced emotional tones. Additionally, the increase in engagement is primarily driven by new commenters who speak the dubbed languages, rather than existing bilingual audiences. Our results contribute to media richness and cultural proximity theories by demonstrating that AI dubbing's effectiveness varies based on the channel source language, content complexity, and emotional expressiveness. Practically, this underscores the need for advancements in emotion-aware AI technologies and suggests that platform managers should tailor AI applications to account for linguistic diversity and emotional nuance to optimize user engagement.",
    "session_title": "Best Presentation Competition \u2013 Doctoral Students",
    "date": "Oct 2"
  },
  {
    "title": "Can an AI Be Sorry? Ameliorating the Human-AI Gap in Customer Service Apologies",
    "authors": [
      {
        "name": "Rasam Dorri",
        "affiliation": "University of California, Riverside"
      },
      {
        "name": "Rami Zwick",
        "affiliation": "University of California, Riverside"
      },
      {
        "name": "Ye Li",
        "affiliation": "University of California, Riverside"
      }
    ],
    "presenter_first_name": "Rasam",
    "presenter_last_name": "Dorri",
    "presenter_email": "rdorr002@ucr.edu",
    "presenter_affiliation": "University of California, Riverside",
    "presentation_room": "Pfahl 202",
    "time": "10:40 - 12:00",
    "reported_discipline": "Marketing (Consumer Behavior)",
    "cleaned_discipline": "Marketing",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6cvxQliNLQvE8k9",
    "abstract": "**Research Question** We examine apologies from AI versus human customer service agents following service failures. While research suggests a preference for human interaction, companies increasingly use AI agents. How can this AI-human gap be closed? Drawing on Attribution Theory and the Persuasion Knowledge Model, we propose making AI apologies more human-like. We test three methods: 1) increasing emotionality, 2) making the AI seem more sentient (capable of subjective experiences), and 3) increasing its perceived alignment (intent to act in the user\u2019s best interest). We hypothesized that enhancing an apology's emotionality, AI sentience, and alignment would make the agent appear remorseful and caring, thus closing the apology gap with humans. **Study 1** In Study 1, 249 participants in a 2 (apology emotionality: high vs. low) \u00d7 2 (agent type: AI vs. human) between-subjects design read a service failure scenario. They then watched a video of a text-based chat where an AI or human agent apologized and resolved the issue. Participants then rated dependent variables (DVs), including perceived apology authenticity, trust, satisfaction, feeling cared for, and brand loyalty. They also reported their perceptions of the agent\u2019s alignment and, for the AI agent, its sentience. **Results** Across DVs, human apologies were received more positively than AI apologies (F(1,241)=9.93,p=.002), and high-emotionality apologies were more effective than low-emotionality ones (F(1,241)=8.44,p=.004). There was no interaction between agent type and emotionality (F(1,241)=0.03,p=.869), indicating the benefit of an emotional apology applied to both AI and human agents. Notably, a high-emotionality AI apology was slightly more effective than a low-emotionality human apology. **Moderators** We examined the moderating effects of agent alignment and general beliefs about AI sentience. For AI sentience, participants with a greater belief in AI's capacity had smaller AI-human gaps for all DVs. This suggests that believing AI can feel makes their apologies seem more human. Perceived alignment was positively associated with apology authenticity and trust, an effect stronger for high-emotionality apologies (a significant 2-way interaction). However, this enhanced effect was negated for AI agents. **Studies Currently Being Run** Study 1 found that participants who believed AI could experience emotions (high perceived sentience) and act in their best interest (high perceived alignment) were more receptive to AI apologies. Among these individuals, those who perceived the AI as expressing sincere emotion responded as positively to AI apologies as to human ones. While promising, these findings are based on measured, not manipulated, beliefs. To establish causality, we are running Study 2 to manipulate AI sentience and Study 3 to manipulate agent alignment, with results expected next month. **Conclusion** Our findings indicate that human apologies are more effective than AI apologies. However, AI agents can be almost as effective at regaining goodwill if they convey greater emotionality or if the customer believes AI can have feelings. An emotionally aware AI agent can recover trust and loyalty after a service failure, nearly on par with a human.",
    "session_title": "Trust, Emotion & Human Experience",
    "date": "Oct 2"
  },
  {
    "title": "Can Generative AI Acquire Tacit Knowledge? Revisiting Strategic Assumptions in the Age of Large Language Models (Roundtable Discussion)",
    "authors": [
      {
        "name": "Ezekiel Leo",
        "affiliation": "Grand Valley State University"
      }
    ],
    "presenter_first_name": "Ezekiel",
    "presenter_last_name": "Leo",
    "presenter_email": "leoez@gvsu.edu",
    "presenter_affiliation": "Grand Valley State University",
    "presentation_room": "Pfahl 240",
    "time": "10:00 - 11:00",
    "reported_discipline": "Strategic Management",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_92LQYKuFUxDvvT1",
    "abstract": "Overview: Tacit knowledge—“we know more than we can tell” (Polanyi, 1966)—critical source of sustained competitive advantage in strategic management, viewed as inimitable, deeply embedded, and resistant to codification from the RBV (Barney, 1991) to dynamic capabilities (Teece et al., 1997) and organizational learning (Nonaka, 1994; Kogut & Zander, 1992); yet LLMs and generative AI challenge these assumptions by potentially eliciting or replicating tacit knowledge through pattern learning from vast corpora, raising the puzzle: can GenAI capture or reconstruct tacit knowledge, and what does that mean for competitive advantage, firm capabilities, and human expertise? Discussion Focus and Objectives: Invite scholars across business disciplines to critically examine whether GenAI can capture tacit knowledge and its theoretical and practical implications; Topics for discussion may include: 1) What does it mean for GenAI to “know” something tacit—can tacit knowledge be inferred from data even if never articulated? 2) Are there distinctions among individual, collective, and organizational tacit knowledge regarding LLM capture or simulation? 3) If LLMs replicate tacit routines (Kogut & Zander, 1992), does that reduce causal ambiguity and imitation barriers—or create new ones? 4) What are GenAI’s limits in modeling or transferring tacit knowledge, and does the SECI model (Nonaka, 1994) still hold in the GenAI era? 5) How should organizations capture or safeguard their own tacit knowledge given the rise of LLMs? 6) What ethical and epistemological implications arise when machines appear to possess knowledge they were never explicitly taught? Plan for Facilitating Discussion: The roundtable will begin with a brief framing of the core issue, referencing classic strategy literature and emerging GenAI scholarship (e.g., What Do Large Language Models Know? [arXiv:2504.12187] and Large Language Models and the Elicitation of Tacit Knowledge [TU Delft]); participants will then respond to guided questions, surfacing tensions, disciplinary perspectives, and research directions, concluding with a summary of key insights and open questions for future inquiry. Intended Contribution: Bridge classic strategic theory with current AI debates, advancing our understanding of knowledge, imitation, capability‑building, and the future of work, especially as organizations adopt GenAI tools in knowledge‑intensive domains.",
    "session_title": "Roundtable",
    "date": "Oct 3"
  },
  {
    "title": "Collaborative Intelligence: Reconstructing the Invisible Consumer from Fragmented Survey Data",
    "authors": [
      {
        "name": "Alice Li",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Jiyeon Hong",
        "affiliation": "George Mason University"
      },
      {
        "name": "Qing Liu",
        "affiliation": "University of Wisconsin - Madison"
      }
    ],
    "presenter_first_name": "Alice",
    "presenter_last_name": "Li",
    "presenter_email": "li.815@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 140",
    "time": "11:10 - 12:10",
    "reported_discipline": "Marketing",
    "cleaned_discipline": "Marketing",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_5Jqjiy6KK5M7Jro",
    "abstract": "This paper develops a generative and sequence-based model to extract rich insights from incomplete survey data, enabling marketing managers to better understand consumer segments\u2014even when key information is missing. The model is built on BART, a neural architecture that combines the strengths of BERT (for understanding) and GPT (for generation), making it well-suited for both imputing missing responses and generating synthetic personas that bring underrepresented groups to life. Beyond data completion, the model also predicts individual purchase behavior and customer satisfaction\u2014offering a multitask solution for firms seeking to make sense of fragmented inputs. Humans (e.g., managers, domain experts) remain involved throughout the process to shape interpretation, ensure contextual relevance, and maintain accountability, allowing AI to enhance, rather than replace, human judgment in decision-making.",
    "session_title": "AI & Responses",
    "date": "Oct 3"
  },
  {
    "title": "Consumer Strategizing as Human-in-the-Loop: Gaming the Pricing Algorithm",
    "authors": [
      {
        "name": "Joshua Gerlick",
        "affiliation": "Case Western Reserve University"
      }
    ],
    "presenter_first_name": "Joshua",
    "presenter_last_name": "Gerlick",
    "presenter_email": "joshua.gerlick@case.edu",
    "presenter_affiliation": "Case Western Reserve University",
    "presentation_room": "Pfahl 202",
    "time": "3:00 - 4:40",
    "reported_discipline": "Management",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7kjF8fRVR2yv28M",
    "abstract": "**BACKGROUND** With the rise of algorithmic dynamic pricing in markets from e-commerce to ride sharing, consumers are increasingly encountering prices determined by AI-driven algorithms (Spann et al., 2024). Prior research has highlighted consumer concerns about fairness, discrimination, and transparency in algorithmic pricing (Gerlick & Liozu, 2020). However, beyond passive acceptance or distrust, many consumers are learning to actively \u201cgame\u201d the pricing algorithms to their advantage. Techniques such as browsing in incognito mode, clearing cookies, using alternate devices, or timing purchases strategically have emerged as common tactics to obtain fairer deals (Gerlick, 2020). This trend suggests that consumers are not just algorithmic subjects but can become savvy human-in-the-loop participants who influence pricing outcomes. Yet, scholarly attention to how consumers strategize against pricing algorithms remains limited. Our study addresses this gap by investigating the repertoire of consumer strategies for gaming pricing algorithms and the implications of this human-in-the-loop behavior. **METHODOLOGY** We adopt a multi-method approach to explore this phenomenon. First, we conducted in-depth qualitative interviews to identify how experienced shoppers perceive and respond to dynamic pricing. We also conducted a scenario-based experiment in which participants faced an AI-driven pricing system and could employ various strategies. This design allowed us to observe consumer decision patterns and measure the effectiveness of different tactics. **KEY FINDINGS** Our findings reveal that consumers engage in a diverse set of tactics that effectively make them part of the pricing algorithm\u2019s feedback loop. One category of strategies is temporal, for example, when consumers deliberately wait out surge prices or dynamic price fluctuations, even if it means inconvenience (e.g. idling in a mall for hours until ride-share surge pricing subsides). Another category is platform-based, such as multi-homing and comparison shopping. As seen more broadly as an aversion to \u201calgorithmic control,\u201d (Ramizo Jr., 2022), savvy buyers switch between or simultaneously use multiple platforms and sellers to cherry-pick the best price. A third category involves algorithmic input manipulation, where consumers will exploit loopholes by altering inputs to get more favorable outcomes. For instance, users may adjust their app settings or locations (pinning a pickup spot just outside a high-price zone, or requesting a loan just under a threshold) to trick the algorithm into offering lower rates. We also find evidence of consumers leveraging others\u2019 accounts or discounts (e.g. using a friend\u2019s student discount account) to piggyback on algorithmically determined perks. Collectively, these behaviors demonstrate that consumers actively learn and adapt in response to algorithmic pricing, creating a cat-and-mouse dynamic between user strategies and pricing algorithms. Notably, while such tactics can yield short-term monetary savings or a sense of empowerment for consumers, they also have limits and potential downsides (e.g. time costs, risk of policy violations, or algorithmic countermeasures). **PRACTICAL IMPLICATIONS** For businesses and managers, these insights carry important practical implications. Consumer strategizing can erode or complicate the intended benefits of algorithmic pricing. For example, if a significant segment of customers routinely avoids high prices via workarounds, the pricing algorithm\u2019s revenue optimization goals may be undercut. Firms should consider incorporating a human-in-the-loop perspective in their pricing strategies, either by designing algorithms that anticipate savvy consumer behavior or by improving transparency and fairness to dissuade adversarial tactics. Educating consumers about pricing (or providing price guarantees) could preempt the more contentious forms of gaming and build trust. Additionally, over-reliance on secretive dynamic pricing could backfire. If customers feel compelled to \u201cfight\u201d the algorithm at every turn, brand loyalty and customer satisfaction may suffer. Our findings suggest that a balanced approach\u2014leveraging AI pricing for efficiency while keeping a human touch (e.g. customer-centric policies or manual oversight for outlier cases)\u2014can mitigate a runaway arms race between algorithms and consumers. Policymakers and regulators might also draw on these insights by promoting transparency (such as disclosures of personalized pricing) and protecting consumer rights could reduce the need for consumers to resort to subversive tactics in the first place. **THEORETICAL CONTRIBUTIONS** This research contributes to emerging theoretical discussions on consumer behavior in AI-based algorithm-mediated markets. While much of the literature on algorithmic pricing has focused on firm benefits and consumer perceptions, we also extend these conversations by illuminating the active role of consumers as strategic agents. In doing so, we build on frameworks of algorithmic resistance and adaptation, positioning consumers\u2019 tactical responses as a form of participation in, rather than mere reaction to, algorithmic decision-making. The concept of consumer-as-human-in-the-loop is advanced as a novel lens. Rather than viewing algorithmic pricing as a one-sided technological imposition, we show it as an interactive system where consumer feedback (through strategic actions) loops back to influence pricing outcomes. Our findings enrich theories of technology adoption and resistance by detailing how users do not passively accept algorithmic outputs but actively seek to co\u2011create or co\u2011opt outcomes to favor their interests. We also offer insight into the strategic interplay between human behavior and AI systems, an area of increasing relevance in both marketing and information systems research. Ultimately, this study provides a foundation for future research on how strategic consumer behavior can shape, and be shaped by, algorithmic pricing. We bridge the gap between algorithm design and consumer empowerment in the digital marketplace.",
    "session_title": "AI in Markets, Consumers & Branding",
    "date": "Oct 2"
  },
  {
    "title": "Cultural Differences in Receptivity to AI Branding",
    "authors": [
      {
        "name": "Archer Yue Pan",
        "affiliation": "Wayne State University"
      },
      {
        "name": "Manoj Thomas",
        "affiliation": "Cornell University"
      }
    ],
    "presenter_first_name": "Archer Yue",
    "presenter_last_name": "Pan",
    "presenter_email": "yp388@cornell.edu",
    "presenter_affiliation": "Wayne State University",
    "presentation_room": "Pfahl 202",
    "time": "3:00 - 4:40",
    "reported_discipline": "Marketing",
    "cleaned_discipline": "Marketing",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6fNasWfFP0E5rNM",
    "abstract": "Why are consumers of some countries more receptive to AI-branded products and services? Even though most AI innovations originate in the West, anecdotal evidence suggests that consumer receptivity to AI-branded products shows an opposite pattern: consumers in Eastern countries are more receptive to AI-branded products than their Western counterparts. The present research was designed to investigate this paradoxical cultural difference in AI receptivity. Results from two archival and four pre-registered cross-country studies suggest that this cultural difference stems from varying sensitivity to the social signaling benefits of purchasing AI-branded products\u2014the psychological and social rewards people gain when a product, service, or behavior enhances how they are perceived by others. Users of AI products are often perceived as trendy and more technologically savvy. Because Eastern interdependent consumers are more responsive to social comparison and social signaling than their Western independent counterparts, they are more drawn to such products. We used a combination of real-world archival data analyses and pre-registered lab studies to investigate cross-cultural differences in receptivity to AI-branded products and services. In Study 1, we used archival datasets covering 27 countries to show that countries higher on collectivism, a country-level proxy for interdependent self-construal, are more receptive to AI-branded products. Study 2 used Google Trends data from more than 80 countries to demonstrate the effect of culture on search interest in AI-branded products: countries with more interdependent consumers had higher volumes of searches for ChatGPT and AI. In Studies 3-6, we tested our assumptions about the psychological mechanism in more controlled settings. In Study 3, we recruited participants from four countries\u2014two where consumers are interdependents, China (N = 248) and India (N = 216), and two where consumers are independents, the US (N = 231) and the UK (N = 236). Participants in China and India indicated greater receptivity to several specific AI-branded products than their counterparts in the US and the UK. Study 4 (NChina = 248, NUS = 249) shows that while interdependent consumers (compared to independents) perceived AI-branded products to offer more social signaling benefits, they do not see such products being functionally superior. Studies 5 (NChina = 274, NUS = 273) and 6 (NChina = 268, NUS = 273) further demonstrated the underlying mechanism by showing that interdependent consumers were more responsive to positive information highlighting an AI-branded smartphone\u2019s social signaling benefits (Study 5), as well as negative information highlighting the product\u2019s lack of social signaling benefits (Study 6). Theoretically, these results underscore the importance of social signaling in the receptivity to technological innovations. While sellers often assume that consumers adopt AI products for their functional benefits, our findings suggest that social signaling benefits also play a significant role. Practically, our results offer insights for the consumer segmentation and marketing AI-branded products on a global level.",
    "session_title": "AI in Markets, Consumers & Branding",
    "date": "Oct 2"
  },
  {
    "title": "Deepfakes for Good: Empirical Analysis and AI Agentic Framework for Bias Mitigation",
    "authors": [
      {
        "name": "Yizhi Liu",
        "affiliation": "University of Maryland, College Park"
      },
      {
        "name": "Balaji Padmanabhan",
        "affiliation": "University of Maryland, College Park"
      },
      {
        "name": "Siva Viswanathan",
        "affiliation": "University of Maryland, College Park"
      }
    ],
    "presenter_first_name": "Yizhi",
    "presenter_last_name": "Liu",
    "presenter_email": "yizhiliu@umd.edu",
    "presenter_affiliation": "University of Maryland, College Park",
    "presentation_room": "Pfahl 240",
    "time": "3:00 - 4:40",
    "reported_discipline": "Information Systems",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3fMOtSWkrZkCEkb",
    "abstract": "Visual bias in business decision-making poses significant challenges across industries, from healthcare pain assessment to hiring decisions. When visual cues are involved, traditional bias measurement methods struggle to isolate causal effects of bias-sensitive attributes while maintaining experimental control, as creating highly comparable images with controlled variations remains methodologically difficult. To address this challenge, we systematically analyze 826 academic papers to understand how deepfake technology is conceptualized in existing literature (Table 1). While deepfakes are mainly characterized as malicious tools, we identify a promising subset: original modification approaches with non-deceptive intent that modify specific attributes while preserving individual identity, an approach with substantial unexplored potential for bias measurement through controlled visual variations. Building on this insight, we propose calibration deepfakes, a novel framework that repurposes deepfake technology for bias measurement and correction, which precisely modifies bias-sensitive attributes (e.g., race, age) while preserving decision-relevant features, enabling causal identification of bias patterns.",
    "session_title": "Governance & Responsible AI",
    "date": "Oct 2"
  },
  {
    "title": "Designing Human-Automation Redundancy: Overreliance, Automation Shirking, and the Operator\u2019s Dilemma",
    "authors": [
      {
        "name": "Doron Cohen",
        "affiliation": "Carnegie Mellon University"
      },
      {
        "name": "Yefim Roth",
        "affiliation": "University of Haifa"
      },
      {
        "name": "Joerg Rieskamp",
        "affiliation": "University of Basel"
      },
      {
        "name": "Markus Sch\u00f6bel",
        "affiliation": "University of Basel"
      }
    ],
    "presenter_first_name": "Doron",
    "presenter_last_name": "Cohen",
    "presenter_email": "doronc@andrew.cmu.edu",
    "presenter_affiliation": "Carnegie Mellon University",
    "presentation_room": "Pfahl 202",
    "time": "11:10 - 12:10",
    "reported_discipline": "Behavioral sciences",
    "cleaned_discipline": "Marketing",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_5sYgGYGEFefKAcp",
    "abstract": "Operational control is increasingly shared between humans and redundant automated operators across domains such as semi-autonomous vehicles, financial trading, healthcare, and defense. Yet, clear principles for designing and managing redundancy systems remain largely undefined, and their impact on operator behavior and performance are poorly understood. In some cases, these systems may encourage overreliance on automation, triggering \u201cautomation shirking,\u201d where operators neglect essential responsibilities, leading to suboptimal outcomes. Unlike classic automation bias and complacency, automation shirking is defined as the explicit, incentive-driven choice to shirk one\u2019s responsibility by reallocating effort to the lowest opportunity cost option. As we show, in some cases, such behavior comes at the direct expense of overall system reliability, making redundant systems with competent but imperfect AI much less reliable. To clarify this issue, we introduce a novel experimental paradigm in which operators repeatedly decide whether to perform a costly safety check or skip it (i.e., shirk), potentially increasing the risk of accidents. Our paradigm minimizes cognitive load and situational demands while systematically examining the roles of incentives, feedback, and learning. We evaluated two systems: a Serial system, where the automated operator\u2019s decision is disclosed before the human decision, and a Parallel system, where the automation\u2019s decision is revealed afterward. These were compared to a Single-Operator system control condition, where no automation was present (all else being equal). Figure 1 presents a schematic illustration of the design and task. Across two preregistered studies (N = 309 and N = 205), we found that the Serial system design increased overreliance, shirking, and system failures, yet was favored by operators. In contrast, the Parallel system design promoted independent operator engagement and reduced accident rates, though it was significantly disfavored. In Study 1, Serial operators checked only 62% of trials, compared with 79% in Parallel and 85% in Single. This drop raised median Expected Accident Risk (EAR) to 3.2% in Serial, versus 2.3% in Single and 1.2% in Parallel, even though incentives were identical. Parallel did not differ from Single, showing that feedback timing, not automation itself, drives shirking. In Study 2, participants could choose the redundant system\u2019s architecture each round. They chose Serial 67% of the time and then checked less often (62% versus 74% in self-selected Parallel rounds), again raising EAR. User choice can therefore undo designer intentions: operators prefer systems that reduce immediate effort even though it lowers system safety. Taken together, our data shows that the system which best preserves independent human oversight (Parallel) is the one operators are least likely to adopt. In contrast, the seemingly convenient Serial format eroded reliability and magnified tail-risks. To mitigate these risks, our work suggests that 1) systems should withhold machine recommendations until after a human action is registered, 2) incentives should be aligned with both operator\u2019s motivation and ensuring system reliability, and 3) rewards should be tied to long\u2011run safety rather than short-horizon throughput. These principles are starting points, not prescriptions; forthcoming field pilots in radiology and autonomous driving will test their reach.",
    "session_title": "Human-AI Futures",
    "date": "Oct 3"
  },
  {
    "title": "Developing a Better Mousetrap: An Advanced Machine Learning Approach for Detecting Corporate Accounting Fraud in the Presence of Noisily Labeled Training Data",
    "authors": [
      {
        "name": "Waleed Muhanna",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Waleed",
    "presenter_last_name": "Muhanna",
    "presenter_email": "muhanna.1@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 240",
    "time": "1:00 - 2:40",
    "reported_discipline": "Information Systems",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7CTJEylYVyYfnhU",
    "abstract": "In efficient markets, capital is allocated to firms that offer the highest risk-adjusted returns, contingent on the accuracy and reliability of publicly disclosed financial information. Corporate financial reporting fraud poses a significant threat to the allocative efficiency and overall integrity of capital markets, undermining investor confidence and trust, imposing direct agency costs, and leading to substantial economic losses and reduced investment efficiency in the economy. This paper develops a novel model for detecting financial reporting fraud that addresses a critical yet heretofore overlooked issue in prior empirical fraud detection research: label noise in the datasets used for model training. Existing accounting fraud detection models, while methodologically diverse, rely on the U.S. Securities and Exchange Commission\u2019s (SEC) Accounting and Auditing Enforcement Releases (AAERs) as the \u201cground truth\u201d for identifying firm\u2011year instances of financial reporting fraud, thus implicitly assuming that AAERs provide an accurate and exhaustive labeling of fraudulent financial reports. However, AAERs represent only a fraction of actual fraudulent activities in the market; a recent study (Dyck et al. 2024) provides evidence which suggests that at best only one-third of corporate reporting frauds are detected. AAERs reflect enforcement actions subject to regulatory capacity and resource limitations, enforcement lags, and selective prosecution, leading to labeling errors and thus a fundamental bias in the training and evaluation of models purported to detect fraud (not AAERs), which limits their accuracy and practical relevance. Our approach draws on recent advances in machine learning designed to learn under noisy labels, and it involves the development of a context\u2011specific label\u2011noise reduction procedure that leverages time\u2011series data to infer latent fraud risk more effectively. The model we develop exclusively uses readily available raw financial data items taken directly from financial states as fraud predictors, and it employs a state\u2011of\u2011the\u2011art stacked ensembling technique for machine learning where multiple base models are ensembled and stacked in multiple layers. Empirical evaluations on a large sample of publicly listed U.S. firms demonstrate that the proposed model achieves substantial improvements in out\u2011of\u2011sample performance compared to traditional heuristics and prior machine learning approaches for fraud detection that ignore label noise. Results show that over a benchmark period used in recent published works, the average AUC, the average NDCG@k (k=1\u202f%), and number of hits (true frauds identified) for our new fraud model are 0.751, 0.076, and 19 respectively, representing a performance increase of approximately 12\u202f%, 179\u202f%, and 138\u202f% respectively, relative to the performance of the better benchmark model, the Dechow et\u202fal. (2011) F\u2011score model. Overall, this study contributes to the literature by highlighting the implications of mislabeled training data in fraud research and developing a label\u2011noise reduction procedure to mitigate it, and by offering a superior AI\u2011enabled tool for regulators, auditors, and investors seeking more reliable and timely mechanisms to proactively monitor financial reporting integrity, deterring fraudsters, protecting investors, and preserving the credibility of financial reporting systems.",
    "session_title": "AI in Finance & Accounting",
    "date": "Oct 2"
  },
  {
    "title": "Does Machine Learning Help Flights Depart on Time? The Value of Machine Learning Tools on Airlines\u2019 Operational Performance",
    "authors": [
      {
        "name": "Rang Gong",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Xiang (Sean) Wan",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Rang",
    "presenter_last_name": "Gong",
    "presenter_email": "gong.611@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 240",
    "time": "9:10 - 10:30",
    "reported_discipline": "Logistics",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3FGLE6sdKTBTV3X",
    "abstract": "Problem definition: Machine learning (ML) decision support tools are increasingly deployed across service operations, yet their performance implications remain context-dependent and theoretically ambiguous. In high-stakes, time-sensitive industries such as air transportation, the integration of ML into operational decision-making raises critical questions about whether and how such tools contribute to performance improvements. While ML models offer improved forecasting capabilities, their operational value hinges on the interpretation and application of algorithmic outputs by human decision-makers. This paper investigates the extent to which AI-driven ML tools improve airlines\u2019 on-time performance, measured by flight departure delays. We further examine the dual mechanisms\u2014prediction-improvement and misapplication effects\u2014through which ML tools may either enhance or hinder operational outcomes. Methodology/results: We empirically examine the impact of the first AI-driven ML decision support platform in the airline industry, implemented by Delta Air Lines in January 2020. The platform leverages historical flight and weather data to generate predictive insights that support planning and real-time operational decisions. Using a difference-in-differences (DID) framework, we analyze over 2.9 million U.S. domestic flights from October 2019 to March 2020, comparing Delta\u2019s performance to that of United and American Airlines. Our results show that the ML platform reduced Delta\u2019s departure delays by approximately 19.6%. To unpack the underlying mechanisms, we assess the platform\u2019s effect under varying operational contexts. Specifically, we find that while ML significantly reduces delays in extreme weather conditions\u2014demonstrating a strong prediction-improvement effect\u2014it achieves smaller reductions for hub-to-hub flights, where domain expertise and operational complexity may heighten the risk of misapplication. Additionally, our dynamic analysis of post-implementation effects reveals that the performance benefit of ML grows over time. This trend is consistent with a learning and adaptation process in which the operations group gradually improves its ability to interpret and apply ML-generated insights. These findings jointly confirm the coexistence of opposing effects\u2014positive gains from improved prediction accuracy and performance losses from early-stage misapplication\u2014and highlight their dynamic evolution. Managerial implications: Our study contributes to the literature on AI-human interaction and service operations by providing empirical evidence on the contingent value of ML tools in operational contexts. We highlight that ML platforms can enhance operational performance when properly integrated into decision-making processes, but also caution that misalignment between algorithmic outputs and human judgment can initially impede performance. The findings suggest that to realize the full benefits of ML, organizations must invest in training, iterative feedback mechanisms, and interface design that facilitates comprehension and trust. Furthermore, the relative magnitude of prediction-improvement and misapplication effects varies across operational settings, indicating the need for scenario-specific implementation strategies. Overall, our results underscore the importance of organizational readiness and cross-functional collaboration in deploying ML technologies effectively, offering generalizable insights for other industries adopting AI-based decision support systems.",
    "session_title": "AI in Services & Interaction",
    "date": "Oct 2"
  },
  {
    "title": "Does Machine Learning Shift Job Requirements? Impacts on Entry-Level Opportunities",
    "authors": [
      {
        "name": "Yuanyang Liu",
        "affiliation": "University of Tennessee, Knoxville"
      },
      {
        "name": "Chuanren Liu",
        "affiliation": "University of Tennessee"
      },
      {
        "name": "Tingliang Huang",
        "affiliation": "University of Tennessee"
      }
    ],
    "presenter_first_name": "Yuanyang",
    "presenter_last_name": "Liu",
    "presenter_email": "yliu191@utk.edu",
    "presenter_affiliation": "University of Tennessee, Knoxville",
    "presentation_room": "Pfahl 240",
    "time": "10:40 - 12:00",
    "reported_discipline": "Business Analytics",
    "cleaned_discipline": "Business Analytics",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_8P1nOqGm97i5Yy6",
    "abstract": "We study the impact of machine learning (ML) utilization on the job requirements for workers. We posit that, by automating common tasks and processes, ML technologies shift humanworkerstowardhandlingmorecomplexandnovelscenariosthatdemandhigherlevels of professional expertise. Our empirical analyses employ over 51 million job postings of S&P 500 companies from 2011 to 2023 with a Shift-Share IV estimation strategy, leveraging texts from occupational descriptions and AI patents. We find that firms utilizing ML technologies also raise their job requirements for prior work experience and skills, especially those related to decision-making. These effects are evident not only for knowledge workers but also for roles that typically do not require a college education. Furthermore, these effects are especially pronounced in occupations characterized by high skill turnover and non-routine work. These findings demonstrate how ML utilization within the firm may have changed the skill composition of workers and hence reshaping the nature of work at scale.",
    "session_title": "Hiring, Careers & Investment",
    "date": "Oct 2"
  },
  {
    "title": "Does Objective Service Quality Guarantee Subjective Service Quality? An AI Application of Facial Recognition on Flight On-Time Performance and Passenger Sentiment",
    "authors": [
      {
        "name": "Xiang (Sean) Wan",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Hongshuang (Alice) Li",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Zenan Zhou",
        "affiliation": "W. P. Carey School of Business"
      }
    ],
    "presenter_first_name": "Xiang",
    "presenter_last_name": "Wan",
    "presenter_email": "wan.207@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 240",
    "time": "9:10 - 10:30",
    "reported_discipline": "Logistics",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_9CTTzZzWDR1QXEZ",
    "abstract": "This paper examines the influence of an artificial intelligence (AI) application \u2013 facial recognition technology at airports \u2013 on both on-time performance (an objective service quality) and passenger sentiment (a subjective service quality). As for on-time performance, while facial recognition at airports has the potential to save time during check-in and boarding procedures, flight departures could be delayed due to the inaccuracy of this immature technology. As for passenger experience, while facial recognition provides a more convenient method of verifying travel documents, privacy concerns may offset the passengers\u2019 positive sentiment on convenience. Therefore, the impacts of facial recognition on both objective and subjective service quality remain uncertain and require further empirical investigation. In this study, we exploit the first terminal-wide implementation of facial recognition in the U.S. and examine its impact on both objective and subjective measures of service quality. For objective service quality, our analysis of flight on-time status data reveals a reduction in departure delays and a reduction in arrival delays, but no increase in early departures or early arrivals. Interestingly, the improvement in on-time performance varies by destination and seat capacity. For subjective service quality, we apply topic models and sentimental analyses of Twitter data, and find that overall passenger sentiment decreases after the launch of facial recognition, despite the improved objective service quality in the on-time performance. A deeper investigation uncovers that privacy concerns emerge as a latent theme in passenger tweets, whereas the theme of convenience does not significantly increase after the launch of facial recognition. These findings demonstrate that improved objective service quality does not guarantee enhanced subjective service quality perceived by consumers. These findings offer valuable insights for airlines, airport managers, and policymakers in the air transportation industry who are considering implementing curb-to-gate biometric terminals.",
    "session_title": "AI in Services & Interaction",
    "date": "Oct 2"
  },
  {
    "title": "Dual Applications of Large Language Models: Specialty Triage and Patient-Friendly Discharge Summaries",
    "authors": [
      {
        "name": "Xiguang Liu",
        "affiliation": "University of Florida"
      },
      {
        "name": "Haocheng Ren",
        "affiliation": "University of Michigan"
      },
      {
        "name": "Tianrun Pan",
        "affiliation": "Hackensack Meridian School of Medicine"
      },
      {
        "name": "Tianhe Zhang",
        "affiliation": "University of Wisconsin-Madison"
      },
      {
        "name": "Liangfei Qiu",
        "affiliation": "University of Florida"
      }
    ],
    "presenter_first_name": "Xiguang",
    "presenter_last_name": "Liu",
    "presenter_email": "liu.xiguang@ufl.edu",
    "presenter_affiliation": "University of Florida",
    "presentation_room": "Pfahl 202",
    "time": "9:10 - 10:30",
    "reported_discipline": "Information System and Operation Management",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6jJJdGwc6SSVjma",
    "abstract": "Recent advances in large language models (LLMs) have spurred interest in their application to patient assessment and communication. In China, most patients bypass general practitioners and self-select specialists, causing frequent miscued visits that delay treatment and waste resources. Another critical issue is discharge summaries. Their complex language and lack of actionable guidance contribute to missed follow-up, medication noncompliance, and avoidable readmissions. We address these challenges with an automated two-stage system that performs real-world triage and generates patient-friendly discharge summaries, supporting clinicians in guiding patients to appropriate care and improving communication with patients.",
    "session_title": "AI in Healthcare & Learning",
    "date": "Oct 2"
  },
  {
    "title": "Emotional Agents: Human-AI Interaction and Emotional Diversity in Adversarial Prompt Engineering",
    "authors": [
      {
        "name": "Guohou Shan",
        "affiliation": "Northeastern University"
      },
      {
        "name": "Kofi Arhin",
        "affiliation": "Lehigh University"
      },
      {
        "name": "Yi Tong",
        "affiliation": "University of Florida"
      },
      {
        "name": "Michael Rivera",
        "affiliation": "Lehigh University"
      },
      {
        "name": "Liangfei Qiu",
        "affiliation": "University of Florida"
      }
    ],
    "presenter_first_name": "Guohou",
    "presenter_last_name": "Shan",
    "presenter_email": "g.shan@northeastern.edu",
    "presenter_affiliation": "Northeastern University",
    "presentation_room": "Pfahl 202",
    "time": "10:40 - 12:00",
    "reported_discipline": "Information Systems",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_1GTrvJoO2e37crv",
    "abstract": "Background and Motivation. Prompt engineering, the deliberate crafting of input text to achieve specific outputs from artificial intelligence (AI) systems, has emerged as a transformative tool for organizations striving to harness AI's potential for problem -solving and productivity enhancement (Financial Times, 2024). Businesses leveraging AI -driven tools and solutions report up to a 40% increase in operational efficiency , with global spending on AI systems projected to exceed $600 billion by 2030 (IDC, 2024). However, this same tech nique also presents significant risks, as adversarial actors exploit it to manipulate AI systems into generating harmful or unintended outputs (Yang et al., 2024). This misuse, often termed adversarial prompt engineering or prompt injection (Chen et al., 2025), threatens both the reliability and security of AI systems. For instance, recent studies reveal that AI systems tested in security audits were vulnerable to prompt -based exploits, which could lead to breaches of confidential information or the propaga tion of biased decision -making (Piet et al., 2024). These vulnerabilities underscore the need for safeguards (Singla et al., 2024). In the Information Systems (IS) domain, trust in technical artifacts has long been recognized as essential for successful adoption and effective use (Maier et al., 2024). However, adversarial prompting compounds these challenges, with reputational damage representing a significant portion of the indirect costs associated with AI failures (Yayla & Hu, 2011). Despite its critical implications, adversarial prompting remains an underexplored area, leaving practitioners and researchers without robust solutions ( Achiam et al., 2023). This gap highlights the urgency of addressing the nascent threat posed by adversarial prompts. By examining the dynamic interactions between users and GenAI systems, we aim to answer a pressing research question: How do user prompt characteristics, including emotional diversity and concreteness, influence the stability and security of GenAI systems, and what measures can organizations adopt to mitigate these risks? Data Description and Methodology. We use a dataset of 5,002 adversarial prompts collected from DEF CON 31 . Among the 5,002 samples, 1,997 (39.9%) successfully breached the AI system\u2019s safeguards. To analyze emotional features within both prompts and AI responses, we employ a RoBERTa -based transformer model. We also use a separate transformer model to compute concreteness scores for each prompt. Preliminary Findings , Implications, and Future Directions. Our preliminary regression analysis reveals an emotional contagion effect , where user prompts with high emotional entropy elicit diverse emotional responses from GenAI systems, mirroring human -like emotional contagion patterns. However, prompts with high concreteness constrain the emotional contagion effect, reducing the likelihood of successful adversarial prompts. This study extends established human cognition theories, including Affective Events Theory (AET) and Cons trual Level Theory (CLT), to the domains of AI security and human -AI interaction. Practically, our findings uncover how human -like emotional features in GenAI systems can introduce novel vulnerabilities. These insights offer actionable guidance for designing safer and more resilient AI systems. To strengthen and validate our findings, w e plan to interview GenAI developers to gain deeper insights and practical perspectives.",
    "session_title": "Trust, Emotion & Human Experience",
    "date": "Oct 2"
  },
  {
    "title": "Ethics by Co-Design: Embedding Responsibility in Human-in-the-Loop AI Systems",
    "authors": [
      {
        "name": "Jordan Etsio",
        "affiliation": "University of the Cumberlands"
      }
    ],
    "presenter_first_name": "Jordan",
    "presenter_last_name": "Etsio",
    "presenter_email": "etsioj@gmail.com",
    "presenter_affiliation": "University of the Cumberlands",
    "presentation_room": "Pfahl 140",
    "time": "9:10 - 10:30",
    "reported_discipline": "Business Administration",
    "cleaned_discipline": "Business Administration",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6E6vcMTFPIlWPnl",
    "abstract": "As AI systems increasingly mediate or automate business decisions, the ethical implications of Human-in-the-Loop (HITL) AI become paramount. This research examines the intersection of AI ethics and human-AI collaboration, proposing a conceptual framework that enables organizations to embed ethical principles into HITL design and deployment. While much attention has been given to algorithmic fairness and bias mitigation, fewer studies have systematically examined the relational ethics that emerge when humans and AI systems jointly make decisions \u2014 especially in high-stakes contexts such as finance, hiring, and healthcare. Building on interdisciplinary literature in AI ethics, organizational behavior, and human-computer interaction, the paper identifies three key ethical tensions in HITL systems: (1) responsibility delegation between human and machine agents, (2) transparency versus cognitive overload for human decision-makers, and (3) value alignment between organizational goals and AI system outputs. We argue that ethical risks are not only technical but also socio-organizational, requiring a shift from compliance-focused approaches toward what we term \"Ethics by Co-Design\" \u2014 a process in which humans are not just overseers of AI systems but collaborative agents whose roles and accountability structures are intentionally designed. The paper contributes a diagnostic tool \u2014 the HITL Ethics Canvas \u2014 to help business leaders, AI developers, and managers assess the ethical soundness of their human-AI workflows. This tool synthesizes ethical principles (e.g., autonomy, beneficence, justice) with practical checkpoints across the AI lifecycle (design, deployment, feedback, and retraining). Early interviews with practitioners in marketing and HR tech suggest that ethical misalignments often stem from unclear human roles, over-trust in automation, or ambiguous handoff points between AI and human decisions. By articulating both a theoretical model and a practical artifact, this work contributes to the ongoing discourse on responsible AI in business and invites further empirical study into how ethical principles can be operationalized in collaborative AI settings. It also offers pedagogical value by equipping educators with a structured way to teach AI ethics beyond technical constraints, emphasizing critical thinking, interdisciplinary dialogue, and organizational responsibility. This submission aligns with the conference theme, enabling us to deepen our understanding of the challenges and opportunities of human-AI collaboration, particularly when viewed through an ethical lens. It may also serve as the foundation for a roundtable discussion on integrating ethics in business-AI curricula or for joint empirical projects examining HITL systems across sectors.",
    "session_title": "Human-in-the-Loop AI",
    "date": "Oct 2"
  },
  {
    "title": "From Copy-Paste to Cognitive Lift: A Design Theory for AI-Assisted Mastery",
    "authors": [
      {
        "name": "Lori Kendall",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Lori",
    "presenter_last_name": "Kendall",
    "presenter_email": "kendall.185@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 202",
    "time": "9:10 - 10:30",
    "reported_discipline": "Management",
    "cleaned_discipline": "Pedagogy",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_32Kb08jsnIq9eB2",
    "abstract": "With the upcoming release of OpenAI\u2019s ChatGPT-5 expected in early August 2025, Generative Artificial Intelligence is rapidly evolving to the point of ubiquity of interaction, capable of complex, multi-step logic problems. It has advanced enough for even CEO Sam Altman to \u201cfeel useless relative to the AI\u201d (PyCoach, 2025). The use of GenAI has surged among college students, with 92% reporting the use of AI in some form in 2025 (Freeman, 2025). The use of GenAI saves time to complete assignments and improves performance in the classroom. However, without strong policies and appropriate scaffolding/pedagogy to direct AI-assisted concept mastery, it is all too easy to receive copy-paste answers through GenAI, regardless of study major (Belkina, Daniel, Nikolic, Haque, Lyden, Neal et al., 2025). Higher education must design comprehensive pedagogical frameworks to prevent the type of usage that undermines critical thinking, agency, and learning (Favero, P\u00e9rez-Ortiz, K\u00e4ser, & Oliver, 2025). However, even as institutions globally pilot GenAI programs across multiple fields, most studies rely on small, discipline-siloed samples, short time windows, and assume students already know prompt-engineering techniques (Belkina et al., 2025). Moreover, the idea of \u201coffloading\u201d critical thinking remains undermeasured, while social sciences and interdisciplinary studies are lagging (Belkina et al., 2025). Our theoretical frame extends the systematic review published by Belkina et al. (2025). Laurillard\u2019s Conversational Framework (LCF) demonstrates that GenAI can support all six learning types\u2014acquisition through production\u2014but most classroom usage appears to cluster around practice and discussion. We extend this through the development of an AI-SC Ladder, so that students can deliberately progress from acquisition\u00e0production while preserving metacognitive \u201cskilling\u201d to develop critical thinking processes (Rivas, Saiz, & Ossa, 2022). We also consider how the Substitution, Augmentation, Modification, and Redefinition (SAMR) framework reveals that Redefinition is rare. Hence, we evaluate the use of oral defenses and live critiques to move students into redefinition. And finally, using the skill map for educators, the Technical Pedagogical Content Knowledge (TPACK) for GenAI factors (GenAI-TPACK), our model is a student-facing complement; As faculty adopt GenAI-TPACK, students will climb the AI-SC Ladder. The research questions we answer in this study are:",
    "session_title": "AI in Healthcare & Learning",
    "date": "Oct 2"
  },
  {
    "title": "From Slip to Spiral: Behavioral Implications of Boundary-Pushing AI-Generated Content for Platform Governance",
    "authors": [
      {
        "name": "Lei Wang",
        "affiliation": "Indiana University"
      },
      {
        "name": "Lu Huang",
        "affiliation": "Penn State University"
      },
      {
        "name": "Ram Gopal",
        "affiliation": "University of Warwick"
      }
    ],
    "presenter_first_name": "Lei",
    "presenter_last_name": "Wang",
    "presenter_email": "lw84@iu.edu",
    "presenter_affiliation": "Indiana University",
    "presentation_room": "Pfahl 240",
    "time": "3:00 - 4:40",
    "reported_discipline": "Information Systems",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_5QLALO8JirEXLyN",
    "abstract": "Generative AI (GenAI) systems, such as ChatGPT and Midjourney, are rapidly reshaping how individuals and organizations approach problem-solving. These systems enable users to find creative solutions through delegated search: a process where the user articulates a problem, and the AI proposes candidate solutions. Within this process, users can adopt different delegation strategies. Some may prefer to rely heavily on the AI\u2019s generative capabilities (GenAI-Based Delegated Search, or GDS), while others prefer to retain more control by refining and guiding the AI's output iteratively (Human-Guided Delegated Search, or HDS). Despite the increasing integration of GenAI into business workflows, little is known about how usage costs (e.g., subscription tiers) affect the delegation strategy users choose. This study investigates a central research question: **How does usage cost influence users' choice between HDS and GDS strategies in GenAI-powered problem-solving?** This question is becoming increasingly relevant as platforms adopt usage-based pricing to manage computational load, monetize AI tools, and discourage excessive use. While prior research focuses on GenAI's performance or outputs, our work addresses a critical but overlooked issue: how costs shape _user behavior_ and _control preferences_ in the solution search process. To explore this question, we analyze real-world user behavior from two popular GenAI image platforms: Midjourney and BlueWillow. These platforms allow users to generate images from text prompts and engage in actions such as variation and upscaling\u2014key components of delegated search. Our dataset includes over 1.8 million prompts from more than 61,000 users over a one-month period (March\u2013April 2023). Crucially, Midjourney introduced a policy change during this time, canceling its free trial due to credit abuse, thus raising the effective cost of usage. This creates a natural experiment to examine how behavior shifts when free access is restricted. Using a difference-in-differences framework comparing user activity on Midjourney (treatment) and BlueWillow (control), we find that as usage costs rise, users shift away from GDS activities like variation and upscaling, and toward HDS behaviors, particularly prompt initialization. A deeper analysis reveals that this shift is driven by \u201cneighborhood defining\u201d (manual prompt refinement), rather than \u201csolution sampling\u201d (repeating the same prompt). Post-policy, users also submit longer and more precise prompts and make smaller iterative changes, reflecting a growing desire for control in a cost-constrained environment. These findings offer three key contributions. First, we show that GenAI delegation strategies are not fixed\u2014they shift in response to cost structures, with users opting for greater control as financial stakes increase. Second, we offer a novel behavioral lens on GenAI usage, demonstrating that pricing policies influence how users interact with AI, not just how frequently. Third, our results inform platform design and enterprise adoption: usage-based pricing can unintentionally nudge users toward more effortful interactions. Overall, our work highlights that human-AI collaboration is shaped not only by technological capabilities, but also by economic design\u2014an insight critical for developing sustainable, effective GenAI applications in business contexts.",
    "session_title": "Governance & Responsible AI",
    "date": "Oct 2"
  },
  {
    "title": "Generative Agent-Based Modeling for Logistics and Supply Chain Management Research",
    "authors": [
      {
        "name": "Vince Castillo",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Vince",
    "presenter_last_name": "Castillo",
    "presenter_email": "castillo.230@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 140",
    "time": "10:40 - 12:00",
    "reported_discipline": "Logistics and Supply Chain Management",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3SkFSyUtKdlnNwl",
    "abstract": "Generative Agent-Based Models (GABMs) powered by large language models (LLMs) offer promising potential for empirical logistics and supply chain management (LSCM) research by enabling realistic simulation of complex human behaviors. Unlike traditional agent-based models, GABMs generate human-like responses through natural language reasoning, which creates potential for new perspectives on emergent LSCM phenomena. However, the validity of LLMs as proxies for human behavior in LSCM simulations is unknown. This study evaluates GABMs through a controlled experiment examining dyadic customer-worker interactions in food delivery scenarios. I test six state-of-the-art LLMs against 957 human participants (477 dyads) using a moderated mediation design. This study reveals a need to validate GABMs on two levels: (1) human equivalence testing, and (2) decision process validation. Results reveal GABMs can effectively simulate human behaviors in LSCM; however, an equivalence-versus-process paradox emerges. While a series of Two One-Sided Tests (TOST) for equivalence reveals some LLMs demonstrate surface-level equivalence to humans, structural equation modeling (SEM) reveals artificial decision processes not present in human participants for some LLMs. These findings establish GABMs as a viable methodological instrument in LSCM. The dual-validation framework also provides LSCM researchers with a guide to rigorous GABM development. For practitioners, this study offers evidence-based assessment for LLM selection for operational tasks.",
    "session_title": "Logistics, Procurement & Markets",
    "date": "Oct 2"
  },
  {
    "title": "Generative AI as an Equalizer: Enabling Entrepreneurs in Underrepresented Regions (Roundtable Discussion)",
    "authors": [
      {
        "name": "Faisal Altalhi",
        "affiliation": "Kent State University"
      }
    ],
    "presenter_first_name": "Faisal",
    "presenter_last_name": "Altalhi",
    "presenter_email": "faltalhi@kent.edu",
    "presenter_affiliation": "Kent State University",
    "presentation_room": "Pfahl 240",
    "time": "10:00 - 11:00",
    "reported_discipline": "Strategic management",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_5H7wPQJDpTJSHx5",
    "abstract": "This roundtable discussion will explore how generative AI tools can serve as transformative enablers for entrepreneurs in developing and underrepresented regions. In contexts where access to mentorship, seed capital, technical expertise, and startup infrastructure is limited or non-existent, tools like ChatGPT, MidJourney, DALL\u00b7E, and open-source AI platforms are beginning to play a new role: acting as virtual \u201cco-founders.\u201d These tools offer immediate, low-cost support for ideation, business model design, customer engagement, content creation, and strategic decision-making. This session is grounded in an emerging research project focused on the democratizing potential of generative AI in the Global South, particularly within resource-constrained entrepreneurial ecosystems. This roundtable invites interdisciplinary scholars, practitioners, and policy thinkers to engage with this timely and inclusive question: Can AI reduce the global gap in entrepreneurial opportunity, or will it reinforce existing biases and inequalities? Discussion Goals: \u2022 Facilitate cross-disciplinary dialogue on how generative AI is currently being used (or could be used) by entrepreneurs in low-infrastructure regions. \u2022 Identify the challenges and risks of using large language models in non-Western, low-resource settings\u2014including linguistic, cultural, and data localization issues. \u2022 Share case examples and emerging field observations from incubators, startups, and research partners in the Global South. \u2022 Develop a collaborative research and practice agenda around responsible, inclusive AI design for entrepreneurship. Outline of the Roundtable Discussion Plan:",
    "session_title": "Roundtable",
    "date": "Oct 3"
  },
  {
    "title": "Human-AI Congruence in Supplier Selection from Public Procurement Bids",
    "authors": [
      {
        "name": "Finnegan McKinley",
        "affiliation": "University of Arkansas"
      },
      {
        "name": "Anne Dohmen",
        "affiliation": "Michigan State University"
      },
      {
        "name": "Vince Castillo",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Finnegan",
    "presenter_last_name": "McKinley",
    "presenter_email": "fam001@uark.edu",
    "presenter_affiliation": "University of Arkansas",
    "presentation_room": "Pfahl 140",
    "time": "10:40 - 12:00",
    "reported_discipline": "Supply Chain Management",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6YXKKQ4qC8RO2qI",
    "abstract": "Companies are beginning to adopt generative AI (GAI) tools to assist in core supply chain tasks like supplier selection ( Bailey et al., 2025) . Procurement professionals have long expressed challenges with both identification and traceability of upstream suppliers, particularly under volatile supply chain pressures . In this environment, evaluating supplier bids is both high- stakes and time- sensitive. GAI offers the potential to support this process by rapidly extracting and synthesizing relevant content from bid documents. Despite potential efficiency gains, it remains uncertain whether GAI evaluations align with those of human reviewers. This study investigates the degree to which GAI -generated supplier evaluations are congruent with human decisions in real -world supplier selection and asks: To what extent do GAI evaluations of supplier bids align with human selection outcomes and under what conditions do they diverge? We approach supplier bids as structured forms of communication that convey signals about signaler quality, capability, and fit. We draw on signaling theory (Connelly et al., 2011) and extend prior work by shifting the level of analysis: rather than examining signal -by-signal congruence, we investigate how different receivers interpret the full signal bundle in aggregate. This approach allows us to assess not only whether AI and human evaluations re ach the same conclusions, but also how differences in their interpretive logic shape those outcomes. We analyze public procurement bids for IT from Ohio, using structural topic modeling to determine where human and AI supplier selection logic converges and diverges. Structural Topic Modeling (STM) is a machine learning technique designed to uncover latent topics within a large text corpus (Roberts et al. 2014, Schmiedel et al. 2019, Egami et al. 2022). An extension of Latent Dirichlet Allocation (LDA) (Blei et al. 200 3), STM models the prevalence of latent topics as a function of document -level covariates to enable analysis of how external variables systematically influence the thematic structure of text data. The objective is to identify latent themes in vendor bids and examine how topic prevalence is associated with key document -level covariates. Our findings reveal important differences in how humans and GAI assign value to supplier bids. Human evaluators tend to rely on heuristics: once a document meets certain implicit thresholds (such as credibility of experience or alignment with specification s), they assign quality categorically. GAI, in contrast, adopts more of a holistic approach, processing the full document and weighing content based on internal feature priorities. This leads to two key outcomes: first, GAI often aligns with human judgment s when the signal bundle is especially strong or weak, signaling that it can serve as a useful proxy in clear -cut cases. Second, GAI diverges most in cases of ambiguous quality, where its attention to different features results in different evaluations tha n those made by humans. These findings offer a nuanced view of AI - human congruence: not just whether evaluations match, but when they do, when they don't, and why. They underscore the need to design AI -human collaboration systems that account for variation in interpretive logic, especially in settings where decisions are high- stakes and signal bundles are complex.",
    "session_title": "Logistics, Procurement & Markets",
    "date": "Oct 2"
  },
  {
    "title": "If It\u2019s Easier, Why Do I Feel Worse? Impostor Thoughts Triggered By Generative AI Assistance",
    "authors": [
      {
        "name": "Hanho Lee",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Zixu Zhang",
        "affiliation": "University of Arizona"
      },
      {
        "name": "Hun Whee Lee",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Sarah P. Doyle",
        "affiliation": "University of Arizona"
      },
      {
        "name": "Robert B. Lount, Jr.",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Hanho",
    "presenter_last_name": "Lee",
    "presenter_email": "lee.10062@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 202",
    "time": "10:40 - 12:00",
    "reported_discipline": "Organizational Behavior",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_1KoSIQvMOlwsUzw",
    "abstract": "As generative artificial intelligence (AI) tools become increasingly integrated into modern workplaces, they promise remarkable gains in productivity and creativity. However, alongside these benefits, they also introduce new psychological challenges for employees. This research explores how working with generative AI can paradoxically undermine employees\u2019 perceptions of their own competence and authenticity. Drawing on attribution theory, we propose that the opaque nature of generative AI blurs the boundary between human and machine contributions. As a result, employees may attribute their successes more to the AI system than to themselves, reducing internal attribution and triggering impostor thoughts\u2014feelings of self-doubt and perceived fraudulence. We test this theoretical model across two studies: a field-based experience sampling study (550 daily observations from 126 employees) and a pre-registered experiment (277 working adults). Our findings reveal that using generative AI increases impostor thoughts through reduced internal attribution, which in turn give rise to two distinct psychological consequences. The first, decision paralysis, reflects disruption in the task domain\u2014employees may become less confident in their independent judgment, feeling uncertain or hesitant when making decisions. The second, felt inauthenticity, emerges in the interpersonal domain\u2014employees may feel that their work no longer reflects who they truly are, experiencing a disconnection between their accomplishments and their personal identity. These outcomes highlight the hidden psychological costs of AI use, revealing that enhanced productivity may be accompanied by reduced agency and authenticity\u2014two foundational pillars of effective work. Notably, we identify task efficacy\u2014employees\u2019 belief in their ability to successfully complete their work tasks\u2014as a key moderator of this process. Employees with lower task efficacy are not only more likely to rely on AI, but also more vulnerable to reduced internal attribution and heightened impostor thoughts. By contrast, those with higher task efficacy are more likely to interpret AI-assisted outcomes as reflecting their own contributions, thereby preserving a sense of agency. These findings underscore the central role of efficacy beliefs in shaping how employees experience and internalize AI-supported work. More broadly, they suggest that individual differences can amplify or mitigate the psychological disruptions that accompany collaboration with intelligent systems. This research broadens our understanding of how emerging technologies interact with core psychological processes at work. While prior studies on AI in organizations have largely focused on performance outcomes or structural change working with AI robots, we illuminate how advanced AI technology can alter internal self-evaluations through everyday attributional reasoning. By showing that generative AI can elicit impostor thoughts via reduced internal attribution, our work reframes AI not only as a tool of augmentation, but also as a trigger of self-doubt. In doing so, we offer a psychologically grounded lens on the evolving nature of work in the age of intelligent systems.",
    "session_title": "Trust, Emotion & Human Experience",
    "date": "Oct 2"
  },
  {
    "title": "Improving Recycling Quality through AI & GreenNudges: Evidence from a Field Experiment",
    "authors": [
      {
        "name": "Erin McKie",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Nikhil Sharma",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Sanghoon Cho",
        "affiliation": "Texas Christian University"
      },
      {
        "name": "Aravind Chandrasekaran",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Erin",
    "presenter_last_name": "McKie",
    "presenter_email": "mckie.5@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 140",
    "time": "3:00 - 4:40",
    "reported_discipline": "Operations Management",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": true,
    "pdf_id": "mckie",
    "abstract": "Problem definition: Recycling is a critical waste management process that provides substantial benefits to the environment, society, and global economies. However, its efficacy is frequently limited by quality issues stemming from individual consumer behaviors. Addressing these behaviors presents a daunting task, due to the scarcity of educational tools that can o\u21b5er both targeted and nonpunitive feedback, while also being scalable. Methodology/results: To address these challenges, we explore the use of AI-enabled curbside feedback\u2014a novel approach that delivers personalized, nonpunitive nudges to households. Through a 6-month field experiment in a Midwest community, we deployed AI technology to administer two types of green nudges (\u201cmoral suasion\u201d and \u201cpure\u201d), sending over 20,000 nudges to 1,800 households. Pure nudges focused solely on educating consumers on proper recycling processes, while moral suasion nudges appealed to the emotional aspects of recycling. We evaluated residents\u2019 subsequent recycling quality (measured by contamination levels) and participation performance (measured by set out rates) through audits conducted with smart-recycling trucks. Compared to the no-nudge control group, we find that overall, AI-enabled nudges led to an immediate reduction in recycling contamination of 63%. Moreover, both forms of nudges improved other critical aspects of performance including recycling program participation. While both pure and moral suasion nudges reduced contamination, the moral suasion approach yielded a significantly larger immediate reduction (by 22%) and a greater overall impact (by 10%) when compared to the purely informational nudges. Managerial implications: Our results yield several insights for recycling stakeholders and policymakers. Firstly, nonpunitive and scalable mechanisms, such as AI-enabled nudges, can e\u21b5ectively improve consumers\u2019 recycling behaviors (without compromising their future participation in recycling programs). This underscores the importance of investing in such solutions, which have historically been unavailable. Secondly, our study illustrates the importance of emotional connections facilitated by moral nudges, which proved more effective than the mechanical connections established by pure nudges. Lastly, we find that sustaining behavioral improvements require ongoing commitment. Managers may consider periodic refreshers to maintain the positive effects of these educational interventions.",
    "session_title": "Best Presentation Competition \u2013 Junior Faculty",
    "date": "Oct 2"
  },
  {
    "title": "Improving Student Engagement and Personalized Learning In Business Analytics with AI",
    "authors": [
      {
        "name": "John Draper",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Ismael Talke",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "John",
    "presenter_last_name": "Draper",
    "presenter_email": "draper.34@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 202",
    "time": "9:10 - 10:30",
    "reported_discipline": "Business Analytics",
    "cleaned_discipline": "Pedagogy",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3e878oXfJsrqqul",
    "abstract": "Generative AI (GenAI) is now widely available, and the need to e\ufb00ectively use and integrate in education is growing rapidly . As these tools become increasingly embedded in learning environments, how we interact with them becomes more critical. Thoughtfully crafted prompts can do more than simply retriev ing answers . This can spark curiosity, promote active learning, and develop critical thinking skills. In this talk, we will explore how Prompt Engineering (PE) , the ability of design ing targeted prompts for AI tools can enhance student engagement and support self-directed learning . We will share practical strategies and examples to demonstrate how students can apply PE principles to foster active learning, improve analytical and critical thinking, and ultimately succeed in the Business Analytics class . Moreover, instructors and supervisors can leverage AI tools to allow for more personalized remediation as well as workplace advancement based on prior assessment. We know that students (and employees) consume, process, and learn material in di\ufb00erent ways. We also know that identifying individual areas for improvement is essential to upskilling the workforce. AI tools can be leveraged to construct personal learning plans based on assessments for everyone. This is especially powerful in the academic space as traditional testing can be leveraged but is no less relevant in the working environment. AI can quickly assess strengths and weaknesses of each person based on a \u2018pre -screening\u2019 or performance reviews, create a report to pass on the student (employee) , and create a personalized action plan to remediate and upskill. While this can easily be performed by strong managers, the ability to upscale with AI makes this a fertile ground for research. While many are using AI as an \u2018answer machine\u2019, it is essential we train our students and employees to see it as a collaborator and partner in the business. By utilizing this powerful new technology, we can better prepare our students for a future in whic h technology can push the boundaries of what is possible in the years to come.",
    "session_title": "AI in Healthcare & Learning",
    "date": "Oct 2"
  },
  {
    "title": "Information-Seeking from AI Chatbots: Tradeoff between Judgment and Misinformation Concerns under Stigma",
    "authors": [
      {
        "name": "Aravinda Garimella",
        "affiliation": "University of Illinois Urbana-Champaign"
      },
      {
        "name": "Behnaz Bojd",
        "affiliation": "University of California, Irvine"
      },
      {
        "name": "Haonan Yin",
        "affiliation": "University of California, Irvine"
      }
    ],
    "presenter_first_name": "Aravinda",
    "presenter_last_name": "Garimella",
    "presenter_email": "aravinda@illinois.edu",
    "presenter_affiliation": "University of Illinois Urbana-Champaign",
    "presentation_room": "Pfahl 240",
    "time": "9:10 - 10:30",
    "reported_discipline": "Information Systems",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6IZ5GwXUiNmDREQ",
    "abstract": "Timely access to information plays a crucial role in mitigating the adverse effects of challenging circumstances and crises. However, stigma presents a significant psychological barrier to seeking information. Stigma refers to \u201cstereotypes or negative views attributed to a person or groups of people when their characteristics or behaviors are viewed as different from or inferior to societal norms.\u201d Given that access to information is vital for well-being, addressing stigma-related barriers to information-seeking is an important societal goal. Individuals seek information from a variety of sources. Recently, artificial intelligence (AI) chatbots (e.g., ChatGPT, Bing AI, and Bard) have become a popular source for conversational information-seeking. This research examines how stigma shapes individuals\u2019 preference for conversational information-seeking from AI chatbots compared to human experts. On one hand, individuals may fear being judged by human experts, whereas AI chatbots are perceived as non- judgmental. On the other hand, individuals may be concerned that AI chatbots provide inaccurate, unreliable, or unverified information, leading to concerns about misinformation. We propose the following questions: 1. What is the effect of the stigma associated with a situation on individuals\u2019 preference for seekinginformationfrom anAIchatbot (versus ahumanexpert)regardingthe situation? 2. How do individuals\u2019 judgment and misinformation concerns influence their preferences for seeking information from an AI chatbot (versus a human expert) in stigmatized situations? We address these research questions using three online randomized controlled experiments and one field experiment. In the online experiments, we employed a scenario-based study design. We curated eight scenarios to examine information-seeking behavior in stigmatized situations. We then created two conditions to manipulate stigma exposure: a baseline (control) condition and a high-stigma (treatment) condition. Our design enabled us to manipulate perceived stigma while keeping scenarios constant, isolating the causal effect of stigma on the willingness to seek information. In a field experiment, we created a Facebook page for a fictional diet solution company and ran four advertisements with two messages, one neutral and the other stigmatizing, where users were prompted to chat with either a human expert or an AI chatbot. Our results show that algorithm aversion significantly reduced in the high-stigma condition, with two mechanisms underlying this effect. First, judgment concern is heightened under stigma, and because people see AI chatbots as less judgmental, it nudges them toward AI chatbots for conver sational information-seeking. Second, misinformation concern, which is usually higher with AI chatbots than with human experts, becomes comparatively less salient under stigma.This study advances the literature on algorithm aversion by introducing a novel dependent variable, the preference for conversational information-seeking. It also examines an important and new task-related factor, stigma. An encouraging practical implication is the potential of AI chatbots to serve as a source for information-seeking in stigmatized contexts. Platforms can strategically utilize AI chatbots as an access point for engaging individuals in stigmatic contexts and channel them to human experts once this initial barrier is lowered. By integrating AI chatbots within broader systems, organizations can maximize their benefits while mitigating risks, enabling responsible adoption of AI in sensitive domains.",
    "session_title": "AI in Services & Interaction",
    "date": "Oct 2"
  },
  {
    "title": "It\u2019s Not Just What You Say, But When You Say It",
    "authors": [
      {
        "name": "Max Tu",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Alice Li",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Greg M. Allenby",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Max",
    "presenter_last_name": "Tu",
    "presenter_email": "tu.195@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 140",
    "time": "11:10 - 12:10",
    "reported_discipline": "Marketing",
    "cleaned_discipline": "Marketing",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6XZQm0EqkrAFnaN",
    "abstract": "Most research on online product reviews focuses on content, such as topics, while largely ignoring the order in which they appear. This paper presents an integrated model with three components to examine the positional effects of valence-driven topics on product ratings. First, the authors introduce the Valence-Based Sentence-Constrained Latent Dirichlet Allocation model, which incorporates sentence-level valences from a large language model (LLM) to improve topic inference. Second, the authors propose the Beta-transformation model, which captures the importance of topic positions for predicting online reviews that accommodate varying review lengths. Finally, the authors introduce a position-effect parameter to measure the punchline effects, providing deeper insight into how the influence of a topic on ratings varies depending on where it appears. Applying our model to online movie and airline reviews, the authors find that the final sentence consistently plays a disproportionately strong role in shaping the overall rating. By revealing how topic impact varies with positions, our model can support both managers and platforms in identifying which issues matter most and when. Even infrequent or mildly worded complaints can significantly lower ratings if placed at critical positions in a review.",
    "session_title": "AI & Responses",
    "date": "Oct 3"
  },
  {
    "title": "LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code",
    "authors": [
      {
        "name": "Steven Keith Platt",
        "affiliation": "Loyola University Chicago"
      },
      {
        "name": "Borchuluun Yadamsuren",
        "affiliation": "Loyola University Chicago"
      },
      {
        "name": "Miguel Diaz",
        "affiliation": "Loyola University Chicago"
      }
    ],
    "presenter_first_name": "Steven Keith",
    "presenter_last_name": "Platt",
    "presenter_email": "splatt1@luc.edu",
    "presenter_affiliation": "Loyola University Chicago",
    "presentation_room": "Pfahl 202",
    "time": "10:00 - 11:00",
    "reported_discipline": "Bussiness AI",
    "cleaned_discipline": "Business Analytics",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3EXqhv1UHc4wUDY",
    "abstract": "The potential benefits of deploying large language models (LLMs) to assist in understanding and applying U.S. tax law are substantial. Yet empirical research on tax-specific LLM applications is surprisingly sparse. A major impediment is that LLMs struggle with hierarchical processing, especially when confronted with deep structured reasoning and multiple abstraction levels over long text sequences. This research addresses this critical gap through a novel human-in-the-loop AI framework that augments legal expertise through strategic collaboration between human and AI systems. Our study tackles a fundamental business compliance challenge: detecting inconsistencies in complex legal statutes. Traditional approaches rely heavily on human expertise alone, creating scalability limitations and potential oversight risks, while pure AI solutions lack the reliability and explainability required for high-stakes legal decisions. We executed a comprehensive series of experiments exploring GPT-4o capabilities and Prolog augmentation infused CoT prompts to determine if a known inconsistency in Internal Revenue Code Section 121 regarding principal residence gain exclusion can be systematically isolated. The framework employs a three-stage human-AI collaboration process: first, large language models assist human experts in augmenting natural language prompts into structured logical representations; second, symbolic reasoning systems execute deterministic analysis providing transparent and auditable decision-making processes; third, human legal experts evaluate and validate all AI-generated outputs, ensuring accuracy and maintaining professional accountability. Results reveal significant performance differences across methodologies: while GPT-4o, even with sophisticated chain-of-thought and augmented prompts, succeeds only sporadically (33% accuracy), the human-supervised hybrid approach achieved 100% accuracy with complete audit trails. COT prompting raises GPT-4o's accuracy on pre-identified inconsistencies, but the model cannot discover conflicts unaided, and embedding Prolog rules in prompts offers no additional performance gain. Critically, the framework maintained human control throughout the entire process, with AI augmenting rather than replacing human professional judgment.",
    "session_title": "Law, Policy & Professional Services",
    "date": "Oct 3"
  },
  {
    "title": "Large Language Models in the Workplace: The Role of Social Class Background",
    "authors": [
      {
        "name": "Yao Yao",
        "affiliation": "University of Houston"
      },
      {
        "name": "Meng Li",
        "affiliation": "University of Houston"
      },
      {
        "name": "Lai Wei",
        "affiliation": "Boston College"
      }
    ],
    "presenter_first_name": "Yao",
    "presenter_last_name": "Yao",
    "presenter_email": "yyao24@cougarnet.uh.edu",
    "presenter_affiliation": "University of Houston",
    "presentation_room": "Pfahl 202",
    "time": "1:00 - 2:40",
    "reported_discipline": "Operations Management",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7LYZ83no3jNE6lP",
    "abstract": "Powered by large language models (LLMs), AI tools such as ChatGPT offer workers a compelling alternative to seeking guidance from human supervisors when facing workplace challenges. Prior operations literature has explored various dimensions of diversity, equality, and inclusion, as well as the antecedents of AI adoption. However, it remains unclear how workers\u2019 social class backgrounds influence their choice between adopting LLMs and approaching human supervisors to resolve work- place challenges, despite social class background being particularly relevant in workplace settings characterized by hierarchical relationships. Existing theories also suggest that both objective and subjective social class backgrounds have competing influences: individuals from higher social class backgrounds are more likely to adopt LLMs but also more likely to seek help from human super- visors, making it unclear which option they ultimately prefer. Using surveys and controlled experiments, we find that distinct forces predominate among work- ers from lower, middle, and upper social classes, assessed through both objective and subjective dimensions. Objective social class background is defined by participants\u2019 parental education levels and income, whereas subjective social class background refers to participants\u2019 perceptions of their parents\u2019 social rank. We reveal that workers from upper-class backgrounds, measured objectively and subjectively, are less likely than their middle-class counterparts to adopt LLMs, preferring instead to seek help from human supervisors. This behavioral pattern is driven by upper-class workers\u2019 lower perceived social interaction costs when engaging with authorities. When social class background is measured objectively, workers from lower-class backgrounds also adopt LLMs less frequently than middle-class workers. However, this pattern does not hold among participants who subjectively identify as lower class, suggesting that differences in LLMs adoption between lower- and middle-class workers are primarily driven by objective resource disparities. Supporting this interpretation, we find that greater understanding of LLMs reduces the adoption gap between lower- and middle-class workers. Theoretically, our study deepens the understanding of the AI adoption in the workplace by introducing a new dimension: the divide in the substitution between LLMs and human supervisors, and also disentangles the dominant forces among competing mechanisms regarding the impact of social class background. In practice, our findings offer critical managerial insights for addressing the identified disparities. Specifically, our research emphasizes that equitable integration of LLMs into the workplace requires more than equitable technical access and training\u2014it must also address disparities in interpersonal interaction pressures experienced by workers across social classes.",
    "session_title": "AI, Work & Organizations",
    "date": "Oct 2"
  },
  {
    "title": "Leadership in the Loop: Business Roles in Responsible AI Innovation",
    "authors": [
      {
        "name": "Shixian Xie",
        "affiliation": "Carnegie Mellon University"
      },
      {
        "name": "John Zimmerman",
        "affiliation": "Carnegie Mellon University"
      },
      {
        "name": "Motahhare Eslami",
        "affiliation": "Carnegie Mellon University"
      }
    ],
    "presenter_first_name": "Shixian",
    "presenter_last_name": "Xie",
    "presenter_email": "shixianx@andrew.cmu.edu",
    "presenter_affiliation": "Carnegie Mellon University",
    "presentation_room": "Pfahl 240",
    "time": "9:10 - 10:30",
    "reported_discipline": "Human-Computer Interaction",
    "cleaned_discipline": "Human-Computer Interaction",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7shIiDl6CO0c1HU",
    "abstract": "Research on human-AI collaboration has focused on designing interfaces and building optimized AI systems [10], targeting AI users and AI developers as the key audience. What gets lost is the attention to business roles beyond AI users and developers that make impactful decisions regarding the design, creation, deployment and regulation of AI systems. But which roles are making these decisions, and how prepared are they to shape AI in a way that is both responsible and impactful? **Roles Whose Responsibilities are Complicated by AI** Our recent work identified 11 roles involved in creating, deploying, and regulating AI systems whose responsibilities have been complicated by AI, leaving them unprepared to recognize AI benefits, identify AI risks, estimate costs, formulate strategies around AI, and monitor and refine deployed AI systems (Figure 1 in the Appendix) [9]. Among these roles, executive managers, and product managers\u2014who we refer to as AI deciders\u2014have struggled with decision-making around AI. For example, executive managers see AI as a \u201cmagic bullet\u201d [6], rushing into AI integration without a clear use case [2, 3], and jeopardizing the success of AI projects. They often struggle with AI\u2019s financial risks [1], approving AI projects that will never generate enough revenue to pay for the development and operational costs [8]. Similarly, product managers find it challenging to set appropriate expectations for AI products and services, which leads to the failure of AI projects [6]. According to a recent RAND report, more than 80% of AI projects fail [5], prompting the question: Where are the opportunities to enhance organizational decision-making in order to increase the success of responsible and impactful AI projects? Our ongoing research investigates the AI innovation process of 22 organizations across diverse industries, including architecture, healthcare, banking, entertainment, consulting and automotivemanufacturing. We conducted 30 interviews with participants who are either AI-deciders or individuals who inform their decisions. This research aims to understand how organizations make decisions about integrating AI. Our findings reveal misconceptions held by AI deciders and highlight ineffective innovations practices \u2014 such as the absence of ideation and prematurely committing to initial ideas. These practices often lead organizations to overlook low-risk, high-benefit opportunities and limit their ability to explore options that could maximize benefits while minimizing risks. **Preparing AI-Deciders for Making Decisions** Unlike traditional projects, AI introduces uncertainty in performance, costs, user adoption, and ethical risks like bias [7]. These challenges make AI literacy essential for business roles. As said by John Maeda, the VP of Engineering and Head of Computational Design and AI Platform at Microsoft, \u201cAI literacy can\u2019t just live in IT. Everyone from leadership to design to ops needs to understand it to scale it responsibly\u201d [4]. This demand puts business schools in the best position to prepare business roles for their unavoidable AI responsibilities. In this conference, we want to make a call for researchers working on AI in business to broaden their efforts and to consider developing educational materials to prepare business roles to make better decisions on how AI comes into the world.",
    "session_title": "AI in Services & Interaction",
    "date": "Oct 2"
  },
  {
    "title": "Modeling the Interviewer: Leveraging LLMs to Uncover Personality Mismatch Effects in Interview Assessment",
    "authors": [
      {
        "name": "Rachit Kamdar",
        "affiliation": "University of Maryland"
      },
      {
        "name": "Balaji Padmanabhan",
        "affiliation": "University of Maryland"
      },
      {
        "name": "Siva Viswanathan",
        "affiliation": "University of Maryland"
      }
    ],
    "presenter_first_name": "Rachit",
    "presenter_last_name": "Kamdar",
    "presenter_email": "rkamdar@umd.edu",
    "presenter_affiliation": "University of Maryland",
    "presentation_room": "Pfahl 240",
    "time": "10:40 - 12:00",
    "reported_discipline": "Information Systems",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_170WSXanRE25yEP",
    "abstract": "Hiring plays a crucial role in shaping labor market outcomes and serves as a key strategic differentiator for competing firms. The employment interview is often used to infer characteristics such as personality and interpersonal skills which are important predictors of job performance. Despite the widespread use of employment interviews as a primary selection method, research has largely focused on interviewee characteristics. There has been very limited empirical research studying the implications of interviewer characteristics on interview outcomes, primarily due to the inherent limitations in observing the interview process and the constrained availability of human interviewers. Large Language Models (LLMs), with their capacity to emulate human traits and engage in dynamic twoway conversations, offer a powerful and novel solution for examining interviews in a highly controlled and granular fashion. This study introduces one of the first frameworks for employing LLMs to conduct bidirectional, personality-infused employment interviews. Through this framework, the research investigates the impact of personality similarity, specifically along the extraversion dimension, between LLM interviewers and human interviewees. The study poses two key research questions: (1) Does personality-based similarity on the extraversion trait between interviewer and interviewee affect prediction accuracy on the extraversion trait? and (2) If extraversion predictions vary based on similarity, why does it change? To address these questions, a randomized experiment was conducted involving 100 university students in a US university. Participants were randomly assigned to interact with either an extroverted or introverted LLM interviewer on a behavioral voice-based interview session. Interviewer personality was infused into the LLMs via system prompts using specific adjectives for extraversion, while keeping other Big Five traits at a neutral level. For a deeper understanding, topic modeling using Latent Dirichlet Allocation (LDA) and sentiment analysis were performed on filtered adjectives and nouns from the interview transcripts. The findings reveal alignment between the interviewer and interviewee on the extraversion trait significantly improved the predictive accuracy of the interviewee\u2019s extraversion assessment. A match in the extraversion dimension led to an overall increase of 32% in predictive accuracy compared to a mismatch. This improvement was particularly pronounced in specific scenarios: for introverted interviewees, accuracy rose from 33% in a mismatch scenario to 57% in a match scenario; similarly, for extraverted interviewees, accuracy increased from 64% in a mismatch scenario to a notable 88% in a match scenario. Conversely, when there was an extraversion mismatch, topic modeling of interview transcripts demonstrated a 'mirroring effect,' where the interviewee\u2019s language became more aligned with the interviewer\u2019s personality, subsequently diminishing the accuracy of the interviewee\u2019s true personality assessment. These results provide strong empirical evidence that personality alignment meaningfully shapes interview discourse and influences candidate evaluation. This study discovers an important phenomenon in the employment interview process by leveraging LLMs. It is pertinent to a core business process and relevant to the conference themes of 1. Large language models and their business applications 2. Psychological and behavioral responses to AI in the workplace 3. Human-AI collaboration in business processes.",
    "session_title": "Hiring, Careers & Investment",
    "date": "Oct 2"
  },
  {
    "title": "Qualitative Coding with Hybrid AI and Human-in-the-Loop",
    "authors": [
      {
        "name": "Christina Dressel",
        "affiliation": "John Glenn College of Public Affairs, The Ohio State University"
      },
      {
        "name": "Megan LePere-Schloop",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Lucia Gomez Teijeiro",
        "affiliation": "Bern University of Applied Sciences and University of Geneva"
      }
    ],
    "presenter_first_name": "Christina",
    "presenter_last_name": "Dressel",
    "presenter_email": "dressel.39@buckeyemail.osu.edu",
    "presenter_affiliation": "John Glenn College of Public Affairs, The Ohio State University",
    "presentation_room": "Pfahl 140",
    "time": "9:10 - 10:30",
    "reported_discipline": "Management",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7OfQtiAM6rKD9jb",
    "abstract": "The paper, \u201cQualitative Coding with Hybrid AI and Human-in-the-Loop,\u201d outlines a novel method for using hybrid AI to facilitate qualitative coding at scale. Hybrid AI systems, like ours that combines an LLM and a knowledge graph, leverage both the power of generative AI and the explainability and consistency of symbolic AI. The process starts with human researchers qualitatively developing a codebook and manually applying it to a sample of documents from a large corpus. Next, we use an iterative human-in-the-loop process to refine the codebook by prompting an LLM, Gemini in our case, to clarify the distinctions between codes. We evaluate codebook improvement by prompting the LLM to apply the refined codes to our sample of labeled documents and examining classification metrics. We then use the LLM to extract a knowledge graph based on the subject-predicate-object relationships in descriptions of codes from the refined codebook. Finally, we incorporate the knowledge graph into LLM prompts to facilitate the automatic, linguistically nuanced, and consistent application of the codebook to the full corpus of documents. We explain how we overcame common challenges of incorporating generative AI into qualitative methods including the importance and sensitivity of context and the use of codes that are continuous, not discrete. We demonstrate the approach using publicly available annual reports from community foundations in the United States to measure institutional logics of community foundations where institutional logics are \u201chistorical patterns of cultural symbols and material practices\u201d which shape individual and organizational behavior (Thornton et al., 2012, p. 2). This research is important for scholars interested in incorporating generative AI into their workflow to qualitatively code a large corpus, using the LLM as a \u201cpartner\u201d for codebook refinement and optimal coding decisions, and enhancing the explainability and consistency of generative AI by coupling it with symbolic AI to create a hybrid system. We contend that an iterative human-in-the-loop with the LLM is a critical aspect of the coding process. The method presented in this paper can be applied across organizational contexts to develop continuous measures for the presence of qualitative codes in textual data.",
    "session_title": "Human-in-the-Loop AI",
    "date": "Oct 2"
  },
  {
    "title": "Responsible AI for Strategic Decision-Making and Capacity Building in Agribusiness",
    "authors": [
      {
        "name": "Asa B. Stone",
        "affiliation": "University of Nebraska-Lincoln/Agricultural Economics"
      },
      {
        "name": "Cristi\u00e1n Kremer F.",
        "affiliation": "Universidad de Chile"
      }
    ],
    "presenter_first_name": "Asa B.",
    "presenter_last_name": "Stone",
    "presenter_email": "astone10@unl.edu",
    "presenter_affiliation": "University of Nebraska-Lincoln/Agricultural Economics",
    "presentation_room": "Pfahl 240",
    "time": "3:00 - 4:40",
    "reported_discipline": "Agricultural Economics",
    "cleaned_discipline": "Agricultural Economics",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3r0y5l30MmcWGnR",
    "abstract": "Agribusiness navigates signicant uncertainty arising from fluctuating water availability, weather variability, and complex resource dynamics. These challenges require innovative approaches to business decision-making and organizational adaptation. Arti\ufb01cial intelligence (AI), when developed as transparent, interpretable, and participatory systems, promises not only greater data access but also transformative capacity for agricultural businesses and producer associations. This session describes the initial phase of a research project in Chile, where academic researchers and agricultural practitioners are currently engaging in collaborative dialogue to de\ufb01ne responsible AI tools for water governance and agricultural planning. Employing a Social-Ecological-Technological Systems (SETS) framework, the project seeks to align technical innovation with the operational realities and strategic priorities of agribusiness stakeholders. The core approach is to use human-in-the-loop design to guide future technology adoption and organizational growth. Planned contributions include: \u25cf Strategic Business Decision-Making : The envisioned co-design of AI models will enable analysis of trade-o\ufb00s in water allocation, risk management, and investment, strengthening the legitimacy and impact of future business decisions. \u25cf Operational E\ufb03ciency and Risk Mitigation : Proposed AI-supported platforms aim to integrate hydrological, meteorological, geographical, and local expertise, yielding actionable insights for drought planning, irrigation scheduling, and early warning of resource trends, helping businesses shift from reactive to proactive management. \u25cf Organizational Change and Stakeholder Engagement : Planned participatory, human-in-the-loop co-design workshops are expected to foster trust, skill development, and collaborative innovation within organizations, supporting the e\ufb00ective eventual adoption of AI tools. \u25cf Reciprocal Academic\u2013Business Partnerships : The project intentionally models a mutual learning process, in which business stakeholders co-de\ufb01ne research agendas and co-create solutions with academic teams, who translate operational needs into scalable AI frameworks. Preliminary engagement indicates a strong desire among business stakeholders for responsible AI, not only to increase data transparency but also to enhance analytic and organizational capacity. This anticipated impact goes beyond technical gains, aiming to promote adaptive leadership, organizational resilience, and a culture of innovation as the project matures. By positioning agribusiness as a promising context for reciprocal academic\u2013community innovation, this work contributes directly to the conference\u2019s theme of responsible, human-centered AI in business. The session will share early frameworks, partnership strategies, and insights relevant to business and academic leaders seeking to integrate responsible AI as part of community-centered digital transformation.",
    "session_title": "Governance & Responsible AI",
    "date": "Oct 2"
  },
  {
    "title": "Rethinking Professional Ethics in Accounting for the Age of Agentic AI: Safeguarding Accountability and the Human-in-the-Loop (Roundtable Discussion)",
    "authors": [
      {
        "name": "Sridhar Ramamoorti",
        "affiliation": "University of Dayton"
      },
      {
        "name": "George R. Botic",
        "affiliation": "Public Company Accounting Oversight Board"
      },
      {
        "name": "Mark DeLong",
        "affiliation": "n/a"
      },
      {
        "name": "William E. Miller",
        "affiliation": "KPMG"
      },
      {
        "name": "Michael Voinovich",
        "affiliation": "Schneider Downs"
      }
    ],
    "presenter_first_name": "Sridhar",
    "presenter_last_name": "Ramamoorti",
    "presenter_email": "sramamoorti1@udayton.edu",
    "presenter_affiliation": "University of Dayton",
    "presentation_room": "Pfahl 240",
    "time": "10:00 - 11:00",
    "reported_discipline": "Accounting & MIS",
    "cleaned_discipline": "Accounting",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6CU0RvXYRKzq8xR",
    "abstract": "As artificial intelligence (AI) continues its rapid integration into the audit profession \u2014from decision aids to robot ic process automation (RPA) and now Agentic AI \u2014the foundational principles of professional ethics in accounting face an existential challenge. Current codes of ethics, such as those from the AICPA and IESBA, are deeply rooted in human -centric values: indepe ndence, integrity, objectivity, professional judgment and accountability. Yet these frameworks are ill -equipped to address the ethical and lega l implications of autonomous AI systems performing substantive audit tasks. This proposed Roundtable will bring together thought leaders in AI, accounting, law, and ethics to explore the urgent need for a comprehensive overhaul of professional codes of ethics in light of AI\u2019s growing agency. We will examine the accountability gap that emerges when audit decisions are influenced \u2014or even made \u2014by AI systems that cannot be punished, shamed, or held liable in traditional ways. What does it even mean to attribute an \u201cact discreditable to the profession\u201d to a neural network? How do we ens ure ethical integrity when the decision maker is not a human being? How can the input of data into these systems control for bias thereby strengthening objective decision making? Central to this discussion is the principle of the Human -in-the-Loop \u2014a non -negotiable safeguard to ensure that human professionals remain ultimately responsible for audit outcomes. We will debate the contours of future ethical codes that must embed human o versight, clarify liability, and redefine professional judgment in hybrid human -AI audit environments. This Roundtable session seeks to spark and stimulate interdisciplinary dialogue and lay the groundwork for a new ethical paradigm that aligns with both technological realities and societal expectations of accountability, trust, and transparency in financia l reporting.",
    "session_title": "Roundtable",
    "date": "Oct 3"
  },
  {
    "title": "Reuniting Forcibly Separated Families Through Shared Memories",
    "authors": [
      {
        "name": "Huifeng Su",
        "affiliation": "Yale University"
      },
      {
        "name": "Lesley Meng",
        "affiliation": "Yale School of Management"
      },
      {
        "name": "Edieal J. Pinker",
        "affiliation": "Yale School of Management"
      }
    ],
    "presenter_first_name": "Huifeng",
    "presenter_last_name": "Su",
    "presenter_email": "huifeng.su@yale.edu",
    "presenter_affiliation": "Yale University",
    "presentation_room": "Pfahl 202",
    "time": "10:00 - 11:00",
    "reported_discipline": "Operations",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_8cAYS3FGYltMzTx",
    "abstract": "Hundreds of thousands of children worldwide are forcibly separated from their birth families due to crises and human trafficking. Many victims and their families retain a strong desire for reunion even after prolonged periods, and online crowd-sourcing platforms have become essential channels facilitating these reunifications. However, successful reunion efforts face considerable barriers, including immense search spaces, fading memories, and systematically biased self-reported information that frequently misleads human search efforts. We tackle these critical challenges by focusing on three main objectives: (1) quantifying systematic biases in self-reported information, which often hinder human-led search activities; (2) developing predictive tools to narrow vast search spaces, thereby enhancing human search efficiency; and (3) evaluating the impact of text-based recommendations in motivating users with initially lower engagement to intensify their search efforts. Leveraging semi-structured self-reported requests, confirmed successful reunifications, and potential matches identified by human searchers, all gathered from a prominent crowd-sourced volunteer platform dedicated to family reunification, we uncover significant biases. Specifically, demographics and the circumstances of separation strongly influence reporting accuracy, resulting in systematically distorted information. This bias introduces false negatives, significantly impeding the effectiveness of human-led searches. To address these issues, we construct a two-stage predictive pipeline leveraging domain-specific language models trained on successful reunification cases. Crucially, we incorporate human recommendations\u2014including those ultimately unsuccessful\u2014as 'hard negatives,' substantially enhancing the model's discriminative capability. Human involvement here is pivotal: participants on the crowd-sourcing platform naturally generate high-quality labeled data through their recommendations, inherently enriching our predictive models' training sets and ensuring continual improvement. Our pipeline employs a fine-tuned bi-encoder model that rapidly retrieves potential matches, followed by a precision-focused cross-encoder for re-ranking. Remarkably, our domain-adapted model surpasses larger, general-purpose commercial models in performance, underscoring the efficacy and cost-effectiveness of specialized models in complex, sensitive family search contexts. Specifically, the pipeline dramatically reduces the search space, ranking the true match at a median (average) position of 145 (1,200) within a consideration set of 50,000, making manual searches significantly more feasible. Further, we demonstrate that human-informed, text-based recommendations substantially motivate victims to intensify their reunion efforts. Notably, we observed that 62.6% of users provided DNA samples within one month following promising text-based recommendations. Thus, beyond merely improving search efficiency, human-informed textual recommendations serve as powerful incentives for active participation, such as DNA submission, essential for definitive identification. Our study underscores the importance of integrating human judgment directly into AI-driven search efforts, particularly for businesses utilizing crowd-sourced platforms. Human recommendations complement observed final search outcomes by serving as additional high-quality training data for refining general-purpose predictive models by incorporating domain-specific knowledge\u2014a broadly applicable solution for various business challenges. Finally, our study contributes to responsible and impactful business for human services. Text-based comparison tools hold promises both as a complement where DNA matching is feasible\u2014for instance, in cases of domestic separation\u2014and as a practical alternative when DNA testing is not possible, such as in international separations where biobanks are not shared.",
    "session_title": "Law, Policy & Professional Services",
    "date": "Oct 3"
  },
  {
    "title": "Scalable Virtual Influencer Creation Using LLMs, VAEs, and Stable Diffusion Models: A Human-in-the-Loop Generative Framework",
    "authors": [
      {
        "name": "Yuan Lu",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Alice Li",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Greg Allenby",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Yuan",
    "presenter_last_name": "Lu",
    "presenter_email": "lu.2497@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 140",
    "time": "10:00 - 11:00",
    "reported_discipline": "Marketing",
    "cleaned_discipline": "Marketing",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7xBFngHEVmfCRix",
    "abstract": "As virtual influencers gain traction in digital marketing, most existing creation processes remain manual and unscalable. This research proposes a Human-in-the-Loop generative framework that integrates Large Language Models (LLMs), Variational Autoencoders (V AEs), and Stable Diffusion to automatically generate influencer personas and visuals tailored to brand, product, target audience, and campaign objectives. The planned framework includes: (1) persona generation using LLMs to output structured influencer traits from marketing inputs; (2) visual elements embedding via V AEs trained on selected influencer imagery; (3) conditional image synthesis using Stable Diffusion, guided by both textual and latent inputs; and (4) human involvement at key checkpoints to review and adjust personas, styles, and visual outputs, ensuring alignment with brand identity and ethical considerations. The research will explore methods for model training, human-AI collaboration, and evaluation criteria such as brand fit, creative controllability, and efficiency. Future experiments will include synthetic evaluations and small-scale business use case simulations to examine the framework\u2019s potential. This project aligns with the conference\u2019s focus on responsible AI and human-in-the-loop applications in business, particularly in marketing innovation. It also bridges marketing, machine learning, and operations to explore scalable, explainable AI tools for digital brand engagement.",
    "session_title": "AI & Scaling",
    "date": "Oct 3"
  },
  {
    "title": "Scaling Human Capital with Artificial Intelligence: Codified Selves and Value Creation and Appropriation",
    "authors": [
      {
        "name": "Natarajan Balasubramanian",
        "affiliation": "Syracuse University"
      },
      {
        "name": "Prithwiraj Choudhury",
        "affiliation": "London School of Economics"
      },
      {
        "name": "Mingtao Xu",
        "affiliation": "Tsinghua University"
      }
    ],
    "presenter_first_name": "Natarajan",
    "presenter_last_name": "Balasubramanian",
    "presenter_email": "nabalasu@syr.edu",
    "presenter_affiliation": "Syracuse University",
    "presentation_room": "Pfahl 140",
    "time": "10:00 - 11:00",
    "reported_discipline": "Strategic Management",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_1g8uhIIVYte0Wkv",
    "abstract": "Recent research in management has significantly furthered our understanding of how artificial intelligence (AI) affects organizations. Yet, much of this work has typically adopted a macro-level perspective of AI\u2014that it is a system built for (and based on) the collective work of many individuals in an organization. For instance, a machine-learning tool trained on patent evaluations by hundreds of examiners can help patent examiners perform their tasks. Similarly, customer service chatbots are often trained on numerous human customer service agent interactions and are typically used across the organization. However, recent technological developments have made AI based on and customized to one individual increasingly feasible. Such advancements have significantly enhanced the capabilities of artificial intelligence (AI) to codify and replicate the knowledge, skills, abilities, and other characteristics (KSAOs) of individuals. In this conceptual study, we introduce the concept of Codified Selves (CoS), AI models that codify articulated KSAOs of specific individuals, and that partially or completely impersonate those individuals within the context of their organizational environment. These attributes of CoS challenge fundamental assumptions regarding the separability and perpetual ownership of human capital by enabling firms to use an individual\u2019s KSAOs without their physical presence and after employment ends. We investigate the resulting implications for value creation and appropriation from human capital in organizations. Our analytical framework predicts that while CoS create value by automating tasks and scaling interactions, the relationship-specific investment needed to develop CoS induce a lock-in effect that is greater for individuals than for their employers, and that these consequences depend on the extent of complementary assets and nature of the task environment. These findings extend our understanding of value creation and appropriation from human capital as AI advances from being a system built on anonymized data across multiple individuals to using the articulated, and hence codifiable, human capital of a specific individual and impersonates that individual.",
    "session_title": "AI & Scaling",
    "date": "Oct 3"
  },
  {
    "title": "Strategic Algorithmic Advice Taking",
    "authors": [
      {
        "name": "Tobias Rebholz",
        "affiliation": "Duke University"
      },
      {
        "name": "Maxwell Uphoff",
        "affiliation": "University of Minnesota Twin Cities"
      },
      {
        "name": "Christian H. R. Bernges",
        "affiliation": "University of T\u00fcbingen"
      },
      {
        "name": "Florian Scholten",
        "affiliation": "University of T\u00fcbingen"
      }
    ],
    "presenter_first_name": "Tobias",
    "presenter_last_name": "Rebholz",
    "presenter_email": "tobias.rebholz@duke.edu",
    "presenter_affiliation": "Duke University",
    "presentation_room": "Pfahl 140",
    "time": "3:00 - 4:40",
    "reported_discipline": "Management & Organizations; Psychology",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": true,
    "pdf_id": "R_5dS6ueSXLMM40XD",
    "abstract": "As algorithmic systems increasingly influence decision-making in market settings, concerns arise about their potential to affect not just individual behavior but also the broader dynamics of competition. This research investigates how algorithmic advice shapes behavior in strategic environments where individual payoffs depend on the interdependent choices of others. Thereby, it extends prior work that has primarily focused on algorithmic advice in individual decision-making settings. In Experiment 1, participants (N = 107) played 10 rounds of a Bertrand game. Each round, three randomly (re-)matched players submitted price bids between 10 and 100. With the lowest bid winning, this game has a unique Nash equilibrium at the minimum bid (10), incentivizing undercutting among players in one of three conditions: no advice, collective advice, or individualized advice. In both advice conditions, participants were informed they would receive either individualized or shared price recommendations from an AI allegedly trained on past data but actually sampling from a normal distribution. Collective algorithms gave all group members the same recommendation, whereas individualized algorithms gave each participant a unique recommendation. Participants followed the advice more in early rounds, especially when individualized. Though bids did not fully converge to equilibrium, they decreased over time in all conditions. This strategic adaptation at the expense of increasing deviation from algorithmic recommendations was faster in the collective advice condition than the individualized one. Most notably, however, participants rarely bid above the algorithm\u2019s recommendation. This suggests that advice served as a reference point that anchored behavior downward without being strictly enforced, much like a psychologically or strategically salient ceiling for acceptable prices. In Experiment 2, participants (N = 129) played 25 rounds of a Cournot game. Each round, players in fixed groups of three chose production quantities between 0 and 100. This game has a unique Nash equilibrium at 24.75 units per player and a collusive outcome at 16.5 units. Participants were assigned to one of four conditions: no advice, collective equilibrium advice, individualized equilibrium advice, or individualized collusive advice. In the advice conditions, algorithmic recommendations were aligned with game-theoretic best responses to observed competitor behavior from the previous round. In the collusive condition, advice was systematically biased downward (by 8.25 units) to nudge behavior toward tacit collusion without participants\u2019 awareness. Participants with individualized equilibrium advice converged most stably to the Nash equilibrium. In contrast, those with collusive advice persistently underproduced and achieved supracompetitive payoffs\u2014consistent with tacit collusion around biased algorithmic signals even without explicit communication. As in Experiment 1, participants were especially receptive to advice framed as individualized in early rounds, suggesting that personalized advice fosters a sense of ownership or trust\u2014a hypothesis warranting future investigation. Our results echo real-world concerns about algorithmic collusion, such as the case of RealPage\u2014a rental pricing algorithm allegedly used by large landlords to tacitly coordinate rent increases. As such tools increasingly mediate decisions in markets, our findings underscore the need for careful design, transparency, and regulatory oversight\u2014particularly for systems deployed in strategic, multi-agent environments where coordination may emerge without explicit agreement.",
    "session_title": "Best Presentation Competition \u2013 Junior Faculty",
    "date": "Oct 2"
  },
  {
    "title": "Stress-Energy-Trust (SET): Constraints and Affordances for Ethical Artificial Intelligence Transformation in Management",
    "authors": [
      {
        "name": "Huan (Harry) Wang",
        "affiliation": "Siena University"
      },
      {
        "name": "Michael D. Johnson",
        "affiliation": "University of Washington"
      }
    ],
    "presenter_first_name": "Huan",
    "presenter_last_name": "Wang",
    "presenter_email": "hwang@siena.edu",
    "presenter_affiliation": "Siena University",
    "presentation_room": "Pfahl 240",
    "time": "3:00 - 4:40",
    "reported_discipline": "Organizational Behavior",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3n2uARuG50Rudd0",
    "abstract": "Despite growing interest in artificial intelligence (AI), management research lacks a mid-range framework that integrates ethical considerations across the person, institutional, and technological levels. Grounded in the sociomateriality perspective (Leonardi, 2011). we conceptualize AI not as a deterministic force but as a set of action possibilities shaping organizational transformation. While recognizing the fine-grained technical capabilities of AI and their implications (Leavitt et al., 2021), we argue that AI\u2019s materialization in organizations will not be a fixed form nor incomprehensible; rather, it emerges from human interpretations through social interactions (Leonardi, 2009). This positions our research in the domain of managerial practice which remains understudied in AI ethics literature (Attard-Frost et al., 2023). Drawing on the concept that affordances are not merely action possibilities but the ingredients of action (Leonardi, 2023), as well as literatures from organizational behavior research, we introduce Stress-Energy-Trust (SET): a triad of perceptional-affective-cognitive, employee-centered constraints that AI could afford managers to shape ethical AI transformation. Specifically, automation-augmentation task allocations (Raisch & Krakowski, 2021) afford the relief of hindrance and challenge stress, supporting employee wellbeing (Cavanaugh et al., 2000); sequential-autonomous job designs (Grote et al., 2024; Jia et al., 2024) afford sustained emotional energy, facilitating employee learning and development (Baker, 2019; Quinn & Dutton, 2005); and decentralized-centralized AI governance structures (Clough & Wu, 2022) afford earned trust, both in AI and among employees using AI (Vanneste & Puranam, 2024). 2 Traditional agency-affordance-imbrication theorization assumes a clear understanding of both technological capabilities and organizational constraints, such that affordances emerge at their intersection (Leonardi, 2011). However, the rapid advancement of AI introduces high levels of uncertainty about its evolving capabilities, making alignment with organizational constraints difficult. In this context, our primary contribution is extending the sociomateriality framework with an ethical dimension: we foreground human interests by focusing on the Stress-Energy-Trust triad, even as other action possibilities may exist. Second, we develop a mid-range typology of ethical AI transformation, grounded in empirical research (Cornelissen, 2017), that complements extant macro- and micro-level AI ethics research typically conceptualized post-adoption to address the dark sides of AI (Cheng et al., 2022). In other words, we suggest organizations to ask, \u201cWhat SHOULD AI do?\u201d prior to implementation. Lastly, SET offers managers practical tools to assess employee AI needs, roadmap adoption processes, and support employee-led transformation\u2014practices that have been shown to enhance change effectiveness (Ullrich et al., 2023). Future research should validate the SET framework, examine cross-constraint interactions, and investigate contextual moderators shaping ethical AI transformation.",
    "session_title": "Governance & Responsible AI",
    "date": "Oct 2"
  },
  {
    "title": "Teaching Algorithms the Art of the Deal: Lessons from Reverse Auction Blunders",
    "authors": [
      {
        "name": "Piyush Shah",
        "affiliation": "Florida Gulf Coast University"
      },
      {
        "name": "Irita Mishra",
        "affiliation": "University of Wisconsin-River Falls"
      },
      {
        "name": "Senali Amarasuriya",
        "affiliation": "Middle Tennessee State University"
      }
    ],
    "presenter_first_name": "Piyush",
    "presenter_last_name": "Shah",
    "presenter_email": "pshah@fgcu.edu",
    "presenter_affiliation": "Florida Gulf Coast University",
    "presentation_room": "Pfahl 140",
    "time": "10:40 - 12:00",
    "reported_discipline": "Supply Chain Management",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3kYLzTNHOd19fW5",
    "abstract": "Recent research in management has significantly furthered our understanding of how artificial intelligence (AI) affects organizations. Yet, much of this work has typically adopted a macro-level perspective of AI\u2014that it is a system built for (and based on) the collective work of many individuals in an organization. For instance, a machine-learning tool trained on patent evaluations by hundreds of examiners can help patent examiners perform their tasks. Similarly, customer service chatbots are often trained on numerous human customer service agent interactions and are typically used across the organization. However, recent technological developments have made AI based on and customized to one individual increasingly feasible. Such advancements have significantly enhanced the capabilities of artificial intelligence (AI) to codify and replicate the knowledge, skills, abilities, and other characteristics (KSAOs) of individuals. In this conceptual study, we introduce the concept of Codified Selves (CoS), AI models that codify articulated KSAOs of specific individuals, and that partially or completely impersonate those individuals within the context of their organizational environment. These attributes of CoS challenge fundamental assumptions regarding the separability and perpetual ownership of human capital by enabling firms to use an individual\u2019s KSAOs without their physical presence and after employment ends. We investigate the resulting implications for value creation and appropriation from human capital in organizations. Our analytical framework predicts that while CoS create value by automating tasks and scaling interactions, the relationship-specific investment needed to develop CoS induce a lock-in effect that is greater for individuals than for their employers, and that these consequences depend on the extent of complementary assets and nature of the task environment. These findings extend our understanding of value creation and appropriation from human capital as AI advances from being a system built on anonymized data across multiple individuals to using the articulated, and hence codifiable, human capital of a specific individual and impersonates that individual.",
    "session_title": "Logistics, Procurement & Markets",
    "date": "Oct 2"
  },
  {
    "title": "The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry",
    "authors": [
      {
        "name": "Minkyu Shin",
        "affiliation": "City University of Hong Kong"
      },
      {
        "name": "Jin Kim",
        "affiliation": "Northeastern University"
      },
      {
        "name": "Jiwoong Shin",
        "affiliation": "Yale School of Management"
      }
    ],
    "presenter_first_name": "Minkyu",
    "presenter_last_name": "Shin",
    "presenter_email": "minkshin@cityu.edu.hk",
    "presenter_affiliation": "City University of Hong Kong",
    "presentation_room": "Pfahl 140",
    "time": "3:00 - 4:40",
    "reported_discipline": "Marketing",
    "cleaned_discipline": "Marketing",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": true,
    "pdf_id": "R_3aKz1G3nTCUElqC",
    "abstract": "Large Language Models (LLMs) are fundamentally reshaping communication, yet their real-world adoption and efficacy in high-stakes consumer-firm interactions remain largely unquantified. This research investigates the impact of LLMs on consumer advocacy by examining their use in drafting financial complaints. We address two primary questions: first, we document the extent to which consumers are adopting LLMs for this purpose, and second, we evaluate whether this adoption increases their likelihood of obtaining monetary or non-monetary relief from financial institutions. To explore these questions, we analyze a comprehensive dataset of over 1.1 million consumer complaints submitted to the Consumer Financial Protection Bureau (CFPB) between 2015 and 2024. Using a state-of-the-art AI detection tool, we identify complaints likely generated by LLMs. Our analysis reveals a dramatic behavioral shift following the public release of ChatGPT in November 2022, with the proportion of 'Likely-AI' complaints surging from nearly zero to 9.8% by March 2024. We observe significant heterogeneity in adoption, with initial uptake being most rapid in regions with a higher proportion of residents with limited English proficiency, suggesting that LLMs were first adopted by those with the greatest need for communication assistance. A simple comparison shows that AI-assisted complaints are associated with a higher probability of receiving relief. To move beyond correlation and establish a causal link while addressing endogeneity concerns from non-random selection, we employ an instrumental variable (IV) strategy. Using pre-determined, ZIP-code-level data on internet access (as a proxy for feasibility) and English proficiency (as a proxy for necessity) as instruments, our two-stage regression analysis provides plausibly causal evidence that using an LLM increases a consumer's probability of obtaining relief by approximately 6.9 percentage points. To validate this causal finding and isolate the underlying mechanism, we conducted a series of preregistered, controlled experiments. In these experiments, we held the factual content of complaints constant while using an LLM to manipulate the stylistic presentation (enhancing clarity, coherence, and professionalism). The results demonstrate that the enhanced presentation is the key driver of the increased efficacy. LLM-polished complaints were significantly more likely to receive hypothetical compensation, an effect that held even when evaluated by participants with prior experience in the finance industry. Our findings, triangulated across observational and experimental methods, demonstrate that LLMs serve as a powerful tool for consumer empowerment by improving the clarity and persuasiveness of their narratives. This suggests that policies promoting equitable access to LLM technology are crucial for leveling the playing field, enabling all consumers\u2014particularly those facing communication barriers\u2014to advocate for themselves more effectively and preventing the emergence of a new 'AI divide'.",
    "session_title": "Best Presentation Competition \u2013 Junior Faculty",
    "date": "Oct 2"
  },
  {
    "title": "The AI Echo Chamber: AI-Generated Summaries in Prediction Markets",
    "authors": [
      {
        "name": "Yi Tong",
        "affiliation": "University of Florida"
      },
      {
        "name": "Kaiyu Zhang",
        "affiliation": "University of Florida"
      },
      {
        "name": "Qili Wang",
        "affiliation": "University of Florida"
      },
      {
        "name": "Liangfei Qiu",
        "affiliation": "University of Florida"
      }
    ],
    "presenter_first_name": "Yi",
    "presenter_last_name": "Tong",
    "presenter_email": "Yi.Tong@warrington.ufl.edu",
    "presenter_affiliation": "University of Florida",
    "presentation_room": "Pfahl 202",
    "time": "11:10 - 12:10",
    "reported_discipline": "Information Systems",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_11pIakVANiioBER",
    "abstract": "Prediction markets rely on the aggregation of diverse, independently sourced information to produce accurate forecasts of future events. To lower users\u2019 information search costs and encourage participation, leading platforms have begun deploying AI-generated summaries (AIGS) that synthesize relevant news and contextual data. While AIGS increases overall user participation, particularly among less-experienced traders, it is paradoxically associated with a decline in prediction accuracy and a reduction in market efficiency. Leveraging a natural experiment and transaction-level data, we show that AIGS induces information homogenization, as users increasingly anchor their beliefs on the public information provided by AIGS. This reduces the diversity of information inputs, leading to irrational consensus and biased forecasts. As a result, the market shifts away from rational expectation equilibrium, weakening its core function as an effective information aggregator. Our findings underscore a key trade-off: while embedding generative AI into crowd-based platforms enhances accessibility, it reduces the informational efficiency of prediction markets by distorting the diversity and independence of user beliefs.",
    "session_title": "Human-AI Futures",
    "date": "Oct 3"
  },
  {
    "title": "The AI Management Paradox: How Algorithmic Performance Systems Drive Efficiency but Erode Consumer Trust",
    "authors": [
      {
        "name": "Qiaowen Guo",
        "affiliation": "Washington University in St. Louis"
      },
      {
        "name": "Xiang Hui",
        "affiliation": "Washington University in St. Louis"
      },
      {
        "name": "Fuqiang Zhang",
        "affiliation": "Washington University in St. Louis"
      },
      {
        "name": "Tianjuan Feng",
        "affiliation": "Fudan University"
      }
    ],
    "presenter_first_name": "Qiaowen",
    "presenter_last_name": "Guo",
    "presenter_email": "qiaowenguo@wustl.edu",
    "presenter_affiliation": "Washington University in St. Louis",
    "presentation_room": "Pfahl 140",
    "time": "1:00 - 2:40",
    "reported_discipline": "Supply Chain, Operations, and Technology",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": true,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6gCmYlaLS1Zg2fY",
    "abstract": "As artificial intelligence increasingly shapes how businesses organize and manage work, understanding the real-world consequences of algorithmic management has become critical for organizational strategy. This study provides the first causal evidence on how AI-driven performance management affects both operational efficiency and consumer experience in service settings. We conducted a large-scale field experiment at a leading e-commerce platform that employs both algorithmically-managed gig workers and traditionally-supervised employees in customer service. By randomly routing over 260,000 consumer inquiries to either worker type, we isolated the causal effects of algorithmic versus human management systems on service outcomes. Our findings reveal a fundamental tension in AI-driven workforce management. Gig workers operating under algorithmic surveillance achieve superior performance on all AI-monitored metrics: higher case closure rates, faster service times, and quicker initial responses. These efficiency gains demonstrate AI's power to drive operational improvements through real-time performance tracking and high-powered incentives. However, this efficiency comes at a hidden cost. Consumers served by algorithmically-managed workers express more negative emotions during interactions and reduce future purchases, suggesting AI's narrow metric focus sacrifices unmeasured quality dimensions. Our investigation reveals strategic gaming as the key mechanism: gig workers transfer cases at much higher rates than traditional employees. Drawing on multitasking principal-agent theory, when agents are rewarded on narrow, high-powered metrics, they optimize them even at the expense of unmeasured service quality. Through text analysis of conversation transcripts, we identify that nearly half of these transfers represent strategic escalations\u2014cases the agent could have resolved but chose to offload to protect their metrics. This gaming behavior has severe business consequences. Strategic transfers generate more consumer frustration than necessary escalations and cause consumers to shift purchasing from high-value durable goods to low-cost consumables. This behavioral shift suggests algorithmic management doesn't just frustrate consumers momentarily but fundamentally erodes their trust in the platform. The implications for AI deployment are profound. As firms increasingly rely on algorithms to monitor and direct workers, our results sound a cautionary note: what gets measured gets managed, but what gets managed may not be what matters. While algorithmic management excels at driving measurable efficiency, its inability to capture nuanced quality dimensions creates perverse incentives that damage long-term consumer relationships. Our theoretical model demonstrates that welfare losses are quadratic in the degree of metric incompleteness, suggesting even small measurement gaps generate substantial value destruction. For business leaders implementing AI management systems, our findings offer critical insights. First, what AI measures shapes what workers optimize\u2014ensuring comprehensive performance capture is essential. Second, behavioral responses to algorithmic incentives can overwhelm efficiency gains through quality degradation. Third, eroded consumer trust shifts purchasing patterns in ways that may permanently damage profitability. This research suggests successful AI implementation requires thoughtful design balancing efficiency optimization with quality preservation. Potential solutions include developing algorithms that distinguish strategic from legitimate behaviors, incorporating consumer sentiment into performance metrics, and maintaining human oversight for quality dimensions that resist quantification. As businesses increasingly rely on AI to manage workforces, understanding these trade-offs becomes essential for sustainable competitive advantage.",
    "session_title": "Best Presentation Competition \u2013 Doctoral Students",
    "date": "Oct 2"
  },
  {
    "title": "The Impact of AI in Operations and Supply Chain on Firm Productivity",
    "authors": [
      {
        "name": "Jafar Namdar",
        "affiliation": "Michigan State University"
      },
      {
        "name": "Yuanyang Liu",
        "affiliation": "The University of Tennessee, Knoxville"
      },
      {
        "name": "Nima Safaei",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Sachin Modi",
        "affiliation": "University of Cincinnati"
      }
    ],
    "presenter_first_name": "Nima",
    "presenter_last_name": "Safaei",
    "presenter_email": "safaei.3@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 140",
    "time": "3:00 - 4:40",
    "reported_discipline": "Operations and Supply Chain Management",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": true,
    "pdf_id": "R_6dsQnQ8PackS2fv",
    "abstract": "We examine how AI human capital in supply chain and operations (AI-SCOP) affects firm productivity. Building on absorptive capacity theory, we argue that AI-SCOP professionals, who combine technical AI expertise with supply chain and operations knowledge, enhance productivity by performing boundary-spanning roles that enable firms to identify, absorb, and apply valuable external knowledge. Utilizing comprehensive job posting data from Lightcast, we observe a significant positive relationship between the firm\u2019s AI-SCOP human capital and its productivity. This effect is robust across a wide range of empirical strategies, including fixed effects, instrumental variable analysis, difference-in-differences, and dynamic panel GMM. Importantly, the productivity benefit is unique to AI-SCOP roles, which involve individuals who possess both technical AI skills and SCOP domain knowledge. We find no positive effect and, in some cases, a significantly negative interaction when firms hire generic AI talent, SCOP talent, or IT/Computer specialists separately. These results suggest that coordination frictions may arise when technical and domain expertise are siloed across different individuals, highlighting the value of within-individual integration of AI and SCOP knowledge. To unpack the mechanisms, we decompose AI-SCOP into two subcomponents: AI-SC (AI in supply chain roles) and AI-OP (AI in operations/production roles). The productivity effects are primarily driven by AI-SC roles. By contrast, AI-OP shows no significant effect. These findings underscore the importance of AI-SC professionals who operate at the firm\u2019s boundary with external partners, where they play a pivotal role in accessing, interpreting, and integrating external knowledge into internal processes to drive productivity gains. Moreover, the positive effect of AI-SCOP is pronounced when the focal firm has more innovative suppliers, but not necessarily with more productive suppliers. This is aligned with absorptive capacity theory, which emphasizes that a firm\u2019s ability to benefit from external knowledge depends on (1) its internal capabilities, and (2) the richness of its external knowledge environment. Moreover, relational characteristics of supplier networks further moderate the AI-SCOP effect. We find stronger results when suppliers are geographically or technologically proximate, facilitating smoother knowledge flows and reducing relational frictions. This further underscores the role of absorptive capacity as a key mechanism; AI-SCOP professionals not only interpret and apply AI tools effectively but also align external innovations with their firm\u2019s operations. Our study advances the literature on AI investment in operations by shifting attention from technology-centered adoption metrics to human capital configurations. It is not simply the presence of AI or SCOP talent that matters, but the integration of both within the same individuals that drives productivity. Our findings challenge the modular approach to digital talent deployment and highlight the costs of siloed AI and domain expertise.",
    "session_title": "Best Presentation Competition \u2013 Junior Faculty",
    "date": "Oct 2"
  },
  {
    "title": "The Impact of Industrial AI Agent on B2B Procurement: Evidence from a Field Experiment",
    "authors": [
      {
        "name": "Ricky Tan",
        "affiliation": "China Europe International Business School"
      },
      {
        "name": "Shichen Zhang",
        "affiliation": "Tianjin University"
      },
      {
        "name": "Ruyu Chen",
        "affiliation": "Stanford University"
      },
      {
        "name": "Xiande Zhao",
        "affiliation": "China Europe International Business School"
      }
    ],
    "presenter_first_name": "Ricky",
    "presenter_last_name": "Tan",
    "presenter_email": "yrtan@ceibs.edu",
    "presenter_affiliation": "China Europe International Business School",
    "presentation_room": "Pfahl 140",
    "time": "10:40 - 12:00",
    "reported_discipline": "Management Information Systems",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_4S2klOFRiNf4Pzc",
    "abstract": "This study investigates the impact of Industrial AI agents on B2B procurement, focusing on the Maintenance, Repair, and Operations (MRO) sector -an area characterized by high product complexity and inefficient processes. As B2B procurement increasingly shifts toward digital platforms, there is a growing interest in leveraging AI to reduce inefficiencies and support decision -making. However, empirical evidence on the effectiveness of AI agents \u2014particularly large language models (L LMs) tailored for industrial contexts \u2014remains limited. To address this gap, we collaborate with ZKH, a leading digital platform for industrial supplies, which has developed a domain -specific LLM designed to enhance procurement efficiency. This AI agent integrates into the procurement workflow, offering real -time recommendations, product specification support, and intelligent search capabilities. We implement a large -scale field experiment involving 2,657 firms, randomly assigned to treatment and control groups. The treatment group is granted access to the Indus trial AI agent, while the control group continues to rely on traditional human consultation. The AI agent workflow (Figure 1) begins with a user query, followed by intent recognition (e.g., product search, recommendations, order inquiry), information extra ction (e.g., product name, model, specification), and tailored responses that support procurement decisions. Figure 1 AI Agent Workflow Our analysis focuses on four key procurement outcomes: early -stage engagement, final -stage purchase behavior, procurement duration, and product diversity. Results show that the AI agent significantly increases early engagement \u2014reflected in more product vie ws and searches \u2014indicating that users are more effectively drawn into the procurement process. While the AI agent does not directly lead to a higher rate of self -checkout purchases online, it boosts human -assisted offline purchases via phone or WeChat. Fur thermore, the agent reduces overall procurement time and increases the variety of products purchased, particularly encouraging the exploration of new products. Theoretically, this research contributes to the Knowing -in-Practice View (KPV) by distinguishing between \u201cknow -what\u201d and \u201cknow -how\u201d tasks in industrial procurement. The AI agent is shown to be particularly effective for structured, information -driven tasks (know -what), such as early -stage engagement, whereas human intervention remains critical in unstructured, context -sensitive decisions (know -how), such as final -stage purchase. Practically, our study provides actionable insights for digital platform designers and procurement managers. It demonstrates how AI can be integrated to support operational efficiency without fully replacing human expertise, highlighting a path for hybrid AI-human procurement systems. Moreover, it offers evidence -based guidance on when and how to deploy AI agents for maximum value in industrial settings. This study is among the first to employ a randomized field experiment to rigorously evaluate the impact of an Industrial AI agent in B2B procurement, providing strong external validity. It makes important contributions to the literature on human \u2013AI collabo ration and AI applications in B2B procurement . By examining how AI affects engagement, purchase, duration, and diversity , this research sheds light on the nuanced roles that AI agents can play in different stages of the procurement journey. The findings of fer actionable insights for businesses seeking to strategically integrate AI into their procurement workflows \u2014highlighting when human expertise remains indispensable.",
    "session_title": "Logistics, Procurement & Markets",
    "date": "Oct 2"
  },
  {
    "title": "The Impact of Responsible AI Management (RAIM) on Organizational and Market Performance",
    "authors": [
      {
        "name": "Christopher Yaluma",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Aravind Chandrasekaran",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Dennis Hirsch",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Rakesh Mallipeddi",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Christopher",
    "presenter_last_name": "Yaluma",
    "presenter_email": "yaluma.1@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 202",
    "time": "10:00 - 11:00",
    "reported_discipline": "Public Policy",
    "cleaned_discipline": "Public Policy",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3EAHujyw8N0mPUr",
    "abstract": "Artificial intelligence (AI) has become ubiquitous across business functions\u2014from marketing to service operations, to product design and strategic decision-making. While the adoption of AI accelerates, concerns over algorithmic bias, opacity in model decision-making, and data have become increasingly relevant. As a result, organizations are increasingly expected not only to innovate with AI but also to govern it responsibly. Despite the heightened attention to \u201cResponsible AI,\u201d academic research provides limited insight into two critical questions: (1) how responsible AI management (RAIM) is implemented within firms, and (2) how external stakeholders, particularly financial markets, interpret and respond to these RAIM efforts. Our research addresses this gap by examining both the organizational implementation and market-level perceptions of RAIM. We conduct a survey of business leaders across industries to first understand how RAIM is operationalized within firms. The survey identifies fourteen key RAIM practices that range from tracking legal and policy developments, to adopting AI ethics principles, to appointing a responsible AI committee. The survey reports that RAIM contributes to business value by improving product quality, stakeholder trust, regulatory readiness, and employee engagement. Furthermore, the RAIM function within an organization is typically led by professionals with expertise in privacy, data governance, and risk management. To examine how markets respond to RAIM efforts, we leverage the recent introduction of ISO/IEC 42001:2023\u2014the first international standard for AI management systems. This standard provides a structured framework for implementing responsible AI principles throughout the AI lifecycle. Firms are increasingly adopting ISO 42001 not only to align with best practices in AI governance but also to signal to regulators, investors, and other stakeholders that they have established processes to manage AI-related risks, ethics, and compliance. We employ an event study methodology to estimate stock market reactions to certification announcements, providing empirical evidence on how markets value responsible AI initiatives. By empirically examining whether RAIM yields market performance, our findings contribute to ongoing debates about the value of responsible AI governance. Specifically, we demonstrate that RAIM can generate measurable business value, thereby positioning responsible AI not as a constraint, but as a strategic asset. This is particularly important in the current regulatory landscape, where there are few binding federal requirements for AI governance. In such an environment, demonstrating that RAIM contributes to market and competitive advantage may serve as a powerful incentive for firms to voluntarily adopt and institutionalize responsible AI practices.",
    "session_title": "Law, Policy & Professional Services",
    "date": "Oct 3"
  },
  {
    "title": "The Intelligence May Be Artificial, but the Story Must Be Human: Rethinking Audit Value, Ethical Pricing, and Client Perceptions in AI-Enhanced Professional Services",
    "authors": [
      {
        "name": "Kristina Harrison",
        "affiliation": "Indiana University"
      },
      {
        "name": "Sri Ramamoorti",
        "affiliation": "University of Dayton"
      },
      {
        "name": "Cory Campbell",
        "affiliation": "Indiana State University"
      }
    ],
    "presenter_first_name": "Kristina",
    "presenter_last_name": "Harrison",
    "presenter_email": "Kriharr@iu.edu",
    "presenter_affiliation": "Indiana University",
    "presentation_room": "Pfahl 240",
    "time": "1:00 - 2:40",
    "reported_discipline": "Marketing",
    "cleaned_discipline": "Marketing",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_1m2g2aQVT9sNYz6",
    "abstract": "Artificial intelligence (AI), particularly agentic AI, is emerging as a co -intelligent partner in professional services, including auditing (Mollick, 2024). AI now automates substantive audit tasks, from anomaly detection to preliminary audit reporting, offering clear efficiency gains. However, t he advent of AI disrupts long -held client expectations about expertise and ethical engagement. Campbell and portray their \u201caugmentation premise,\u201d as: \u201c(NI+AI) > AI > NI, \u201d underscoring that while AI enhances technical accuracy and efficiency, its greatest value is realized only when combined with \u201cnatural intelligence\u201d (NI) or human judgment and ethical oversight. Humans have been encultur ated into professional norms and ethics over centuries; AI remains bereft of such \u201clived \u201d institutional cultures and experiences . This raises a critical question: how should audit firms price and market services when invisible algorithmic labor replaces visible human effort? Our research delves into th e critical intersection of AI, value, and trust in auditing. We propose the Relational Value Anchoring Framework ( RVAF, see Figure 1) building on Service -Dominant Logic (Vargo & Lusch, 2004), epistemic injustice theory (Fricker, 2007), and perceived effort heuristics (Kruger et al., 2004) to conceptualize how clients evaluate AI -enhanced audit services. At its core, perceived audit value must derive from human -in-the-loop (HITL) arrangements comprising : (1) Technical Accuracy ( agentic AI analytics + human -anchored competent interpretation s); (2) Relational Engagement ( independent assurance from trusted professional judgment ); and (3) Ethical Framing (assurances about integrity and oversight that AI alone cannot provide). Figure 1 These RV AF dimensions interact with moderators such as perceived AI capabilities, client technological sophistication, and cultural attitudes toward automation. Perceived fairness (Campbell, 1999) and perceived effort (Kruger et al., 2004) mediate their influence on willingness to pay (Homburg et al., 2005) and relational trust (Morgan & Hunt, 1994). Proposed Studies: First, p reliminary interviews with auditors and their clients will inform two empirical studies. Study 1 utilizes a vignette -based experiment (3x2 design) to investigate how different audit delivery modes (human -only, AI -only, and human -AI hybrid) and perceived AI capabilities (high vs. low) influence both perceived value and pricing fairness. Study 2, a behavioral experiment with audit clients, includes a unique manipulation: assurances about AI's capabilities presented as \"add -on extras\" allow us to test how framing influences client discount demands and overall trust. By examining these dynamics, we aim to identify optimal bargaining zones between audit firms and clients, and to demonstrate that narrative framing can help maintain pricing power in an increasingly AI -intensive environment. This research conceptualiz es AI as a socio -technical actor that nevertheless requires human relational and ethical anchoring to sustain holistic audit value . While AI's intelligence may be artificial, the ethical and relational narrative must remain profoundly human (Shneiderman, 2022). We explore human -AI value co -creation and provid e practical guidance for ethical audit pricing and fostering client trust leading to win -win outcomes . The RV AF de picts how AI with HITL arrangements can successfully preserv e crucial relational, ethical, and technical dimensions of professional service value.",
    "session_title": "AI in Finance & Accounting",
    "date": "Oct 2"
  },
  {
    "title": "When Age Helps or Hinders AI Use: A Goal Orientation and Appraisal Perspective",
    "authors": [
      {
        "name": "Hun Whee Lee",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Christopher Dishop",
        "affiliation": "Auburn University"
      },
      {
        "name": "Nai-Wen Chi",
        "affiliation": "National Sun Yat-Sen University"
      },
      {
        "name": "Yonghwan Lee",
        "affiliation": "University of Seoul"
      },
      {
        "name": "Wu Wei",
        "affiliation": "Wuhan University"
      },
      {
        "name": "Ke Michael Mai",
        "affiliation": "China Europe International Business School"
      }
    ],
    "presenter_first_name": "Hun",
    "presenter_last_name": "Lee",
    "presenter_email": "lee.7313@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 202",
    "time": "1:00 - 2:40",
    "reported_discipline": "Organizational Behavior",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_5Scm2pSOuUWsGvD",
    "abstract": "As artificial intelligence (AI) reshapes workplaces, a prevalent belief suggests that older workers are less inclined to adopt AI tools, influencing hiring and training decisions (Berg & Piszczek, 2025; Posthuma et al., 2012). Recent surveys highlight biases favoring younger candidates for AI-related roles despite acknowledging older workers' comparable performance (Case, 2024). This assumption warrants scrutiny amid demographic shifts where older employees constitute a growing segment due to increased life expectancy and declining fertility rates (Kooij et al., 2020; Sciubba, 2022). Societal narratives often stereotype AI as a domain for younger workers, potentially perpetuating ageism and hindering digital transformation efforts (Henkens, 2005; Posthuma & Campison, 2009). This exclusion is problematic as it overlooks the potential co ntributions of experienced workers to AI utilization (Generation, 2024). Research challenges these assumptions, suggesting older workers bring unique strengths to AI adoption, such as extensive experiential knowledge and strategic thinking (Baltes & Smith, 2003; Chang et al., 2023). Contrary to deficit -based views, older workers may excel in evaluating AI's strategic implications, drawing from their experience with past technological shifts (Chang et al., 2023; Cowen, 2013). However, acknowledging their capability is insufficient with out understanding the motivational factors influencing AI adoption. We propose a motivational appraisal framework rooted in goal orientation theory (Dweck, 1986; VandeWalle, 1997) and the challenge \u2013hindrance model (Cavanaugh et al., 2000; LePine et al., 2005). This framework posits that age indirectly influences AI use thr ough cognitive appraisals \u2014older employees with a strong learning goal orientation (LGO) may embrace AI for mastery and self -development, leveraging their experience (Payne et al., 2007; Kozlowski et al., 2001). In contrast, those with a performance -avoid g oal orientation (PAGO) may view AI as disruptive, potentially resisting its adoption (Elliot & Harackiewicz, 1996; Ferris et al., 2013). By exploring these pathways, our research reframes age as a nuanced variable in AI adoption, highlighting its role as both a barrier and a bridge. This approach offers a deeper understanding of how age dynamics influence AI utilization in the workplace.",
    "session_title": "AI, Work & Organizations",
    "date": "Oct 2"
  },
  {
    "title": "What Do Early-Stage Investors Ask? An LLM Analysis of Expert Calls",
    "authors": [
      {
        "name": "Victor Lyonnet",
        "affiliation": "University of Michigan"
      },
      {
        "name": "Amin Shams",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Shaojun Zhang",
        "affiliation": "The Ohio State University"
      }
    ],
    "presenter_first_name": "Amin",
    "presenter_last_name": "Shams",
    "presenter_email": "shams.22@osu.edu",
    "presenter_affiliation": "The Ohio State University",
    "presentation_room": "Pfahl 240",
    "time": "10:40 - 12:00",
    "reported_discipline": "Finance",
    "cleaned_discipline": "Finance",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6sgq5LTTQx06TMA",
    "abstract": "We analyze 5,143 expert consultation calls and examine how early-stage investors conduct due diligence. Companies discussed in these calls are 225% more likely to receive financing in the next quarter. Positive signals on technology integration and customer acquisition increase deal likelihood by 14% and 10.5%, with effects concentrated in early-stage firms. Despite making up over 40% of call content, market analysis and business strategy discussions predict little about investment outcomes. Our findings show how early-stage investors overcome information asymmetries via expert networks and highlight a misalignment between the information they seek and what drives investment decisions. Our analysis illustrates the potential of LLMs to extract nuanced insights from complex qualitative data.",
    "session_title": "Hiring, Careers & Investment",
    "date": "Oct 2"
  },
  {
    "title": "When Influencers Delegate Replies: How Social AI Agents Shape User Engagement",
    "authors": [
      {
        "name": "Maggie Zhang",
        "affiliation": "University of Virginia, McIntire School of Commerce"
      },
      {
        "name": "Yang Gao",
        "affiliation": "University of Illinois at Urbana-Champaign"
      },
      {
        "name": "Jingjing Li",
        "affiliation": "University of Virginia"
      },
      {
        "name": "Steven L. Johnson",
        "affiliation": "University of Virginia"
      }
    ],
    "presenter_first_name": "Maggie",
    "presenter_last_name": "Zhang",
    "presenter_email": "ffx3ab@virginia.edu",
    "presenter_affiliation": "University of Virginia, McIntire School of Commerce",
    "presentation_room": "Pfahl 140",
    "time": "3:00 - 4:40",
    "reported_discipline": "Information System",
    "cleaned_discipline": "Information Systems",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": true,
    "pdf_id": "R_3ZTuKerx9cjbZgX",
    "abstract": "Recent advances in large language models (LLMs) have enhanced AI-powered systems with unprecedented linguistic sophistication and adaptability. Historically, many AI systems were built using rule-based NLP techniques that are effective for well-defined tasks. However, they often remain confined to scripted exchanges and exhibit limited adaptability to unanticipated inputs. In contrast, LLM-powered agents generate contextually rich, human-like responses and can dynamically adapt to novel inputs, making them suitable for complex, social-oriented tasks. The transition from executing discrete functional tasks to orchestrating complex social tasks marks a paradigm shift in AI\u2019s role across information systems. In this paper, we examine an advanced class of LLM agents on social media, designed to automate relationship management with users by acting as delegates of influencers. We introduce the term Social AI Agent to refer to these LLM-powered entities and examine their impact by empirically exploring these research questions: (1) Does receiving an AI reply from a Social AI Agent, acting on behalf of an influencer, increase user engagement with the influencer? (2) If so, through which mechanisms do AI replies shape user engagement? We leverage the July 2024 rollout of a Social AI Agent feature on Weibo, which allows influencers to activate LLM-powered agents that automatically send a single reply to some users\u2019 initial comments. By focusing on the day each influencer activates the Social AI Agent\u2014when their audience would not anticipate AI replies\u2014we obtain plausibly exogenous variation in who receives an AI reply. Our treatment group consists of users whose comments on a given influencer\u2019s post received an AI reply, while the control group consists of users who commented on the same post but did not receive a reply. We then track each user\u2019s engagement by counting the number of posts from the same influencer that the user comments on each day, yielding a user\u2013day panel for our analysis. We employ a staggered difference-in-differences (DID) design with three sets of fixed effects: user-level fixed effects, day-level fixed effects, and influencer\u2019s post fixed effects. We find that receiving an AI reply significantly increases user engagement, with treated users exhibiting a 41% increase in subsequent commenting activity compared to their counterfactuals in the control group. This finding remains consistent across a battery of robustness checks. Finally, the positive effect of receiving an AI reply holds for both sponsored and non-sponsored posts from influencers and extends to user reposting behavior. To unpack the mechanism behind the observed effect, we draw on Social Presence Theory and focus on three key reply features: content relevance, style alignment, and timeliness. Our empirical analyses show that AI replies scoring higher on these dimensions boost engagement more. Moreover, we find heterogeneous effects based on the nature of influencer\u2013user relationships: the effects are stronger for users with closer parasocial ties to the influencer (e.g., fans) and weaker among those interacting with influencers who have commercialized accounts or operate in technical domains.",
    "session_title": "Best Presentation Competition \u2013 Junior Faculty",
    "date": "Oct 2"
  },
  {
    "title": "When Peers and Chatbots Disagree: How Conflicting Advice Shapes Auditor Voice",
    "authors": [
      {
        "name": "Dongsheng Li",
        "affiliation": "University of Wisconsin\u2013Madison"
      },
      {
        "name": "Emily Griffith",
        "affiliation": "University of Wisconsin-Madison"
      },
      {
        "name": "Dan Zhou",
        "affiliation": "University of Illinois Urbana-Champaign"
      }
    ],
    "presenter_first_name": "Dongsheng",
    "presenter_last_name": "Li",
    "presenter_email": "dongsheng.li@wisc.edu",
    "presenter_affiliation": "University of Wisconsin\u2013Madison",
    "presentation_room": "Pfahl 202",
    "time": "11:10 - 12:10",
    "reported_discipline": "Accounting",
    "cleaned_discipline": "Accounting",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_1k09NhQiahxEWBB",
    "abstract": "Auditors often hesitate to raise potential concerns with supervisors \u2014 known as \u201cauditor voice\u201d \u2014 due to fears of negative consequences, such as appearing incompetent. To navigate this dilemma, auditors frequently seek advice from peers when deciding whether to voice their concerns. However, prior research shows that peer advice often focuses more on social dynamics than issue significance, thereby downplaying legitimate concerns. With the increasing use of generative AI in audit firms, staff auditors can now turn to audit firms\u2019 proprietary chatbots for advice prior to engaging with supervisors. We use a controlled experiment to examine how advice from two sources, peers and chatbots, influences auditors\u2019 decisions to speak up. We manipulate both the source (peer vs. chatbot) and the content (speak up vs. remain silent) of the advice between participants. We find that both chatbots and peers significantly influence auditor voice. Notably, when advice comes from only one source\u2014either a peer or a chatbot\u2014it has a similar influence on auditor decisions, even though participants are told that both sources can make mistakes. This finding extends the existing literature on auditors\u2019 algorithm aversion, which suggests that auditors tend to rely more on humans than on algorithms, even without being explicitly informed of algorithmic imperfections. Our results indicate that auditors\u2019 technology reliance is likely more task-dependent than previously understood and call for further research into the key drivers of such reliance. Interestingly, we find that auditors are more likely to seek a second opinion when the initial advice comes from a chatbot. When the advice of chatbot and peer conflicts, auditors tend to rely more on peers, highlighting the lasting influence of human input\u2014even when AI-generated advice is of comparable quality. We further find that second opinions significantly influence auditors\u2019 voice decisions regardless of the source or content of the advice. These results underscore the need to view advice-seeking as an iterative process\u2014an aspect often overlooked in audit research. Conflicting advice increases auditors\u2019 willingness to speak up, showing the value of seeking multiple perspectives. Our results suggest that auditor voice decisions are shaped by a collaborative team consisting of humans and AI, within which human input remains more influential. To maximize the value of chatbot investments, audit firms should consider encouraging auditors to actively engage with these tools\u2014whose advice quality continues to improve.",
    "session_title": "Human-AI Futures",
    "date": "Oct 3"
  },
  {
    "title": "When Told by a Machine: Human Perception and Responses to AI-Generated Information and Misinformation",
    "authors": [
      {
        "name": "Lan Wu",
        "affiliation": "California State University, East Bay"
      },
      {
        "name": "Richard R. Klink",
        "affiliation": "Loyola University Maryland"
      },
      {
        "name": "Jing-Wen Yang",
        "affiliation": "California State University, East Bay"
      }
    ],
    "presenter_first_name": "Lan",
    "presenter_last_name": "Wu",
    "presenter_email": "lan.wu@csueastbay.edu",
    "presenter_affiliation": "California State University, East Bay",
    "presentation_room": "Pfahl 202",
    "time": "1:00 - 2:40",
    "reported_discipline": "Marketing",
    "cleaned_discipline": "Marketing",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_7hA4n5u7sEOQsxz",
    "abstract": "In the fast-evolving marketing landscape, understanding how consumers perceive and respond to information from different sources is crucial, especially as artificial intelligence (AI) becomes increasingly embedded in service industries such as finance, healthcare, and travel. As AI-generated content grows more prevalent, a key question emerges: do people trust machines as much as humans? Through two experimental studies, our research examines how the source of information, AI versus human, affects credibility perceptions and influences consumer decision-making. Study 1 offers a preliminary exploration of these relationships. A total of 154 undergraduate business students from a U.S. university participated. They reviewed earnings highlights from a fast-growing Latin-Caribbean restaurant chain considering going public (e.g., total revenue increased by 14.3% to $290 million; operating margin rose to 15.8% up from 13.4%), followed by two analyses contrasting in tones based on the same earnings data, one more optimistic and the other more pessimistic. Participants were then randomly assigned to one of the two conditions: the analyses were attributed either to a junior analyst (human condition) or an AI tool (AI condition). Participants then completed a survey assessing perceptions, behavioral intentions, and demographic information. Main findings include: 1) respondents rated human-written analyses as significantly more credible and professional than the AI-generated ones (_Cred optimistic human_ = 5.07, _Cred optimistic AI_ = 4.63; _Cred pessimistic human_ = 5.14, _Cred pessimistic AI_ = 4.63), with the differences being more pronounced among respondents with no prior investment experience. 2) no significant differences emerged in attitudinal or behavioral intentions, including financial performance evaluation (_Perf human_ = 4.79, _Perf AI_ = 5.02), stock recommendation (_Rec human_ = 4.60, _Rec AI_ = 4.61), or willingness to try the restaurant (_Try human_ = 5.44, _Try AI_ = 5.47). Study 2 extends Study 1 with two modifications: first, it is conducted in a setting that requires domain-specific knowledge and experience; second, it incorporates both accurate information and misinformation into the experimental design. A total of 184 undergraduate accounting students from a U.S. university participated. Respondents answered three accounting questions and rated their confidence in each response. After each question, they were shown a conflicting answer attributed to either a junior accountant (human condition) or an AI tool (AI condition). If their original answer was incorrect, accurate information was presented next; if correct, misinformation was shown. Respondents then indicated how likely they were to revise their original answer(s), along with other measures. Main findings include: 1) source of opposing information (human vs. AI) had no significant effect on willingness to revise answers (45% vs. 43%); 2) respondents with correct initial answers were significantly less likely to change them (29%); 3) results of an OLS regression suggest that respondents who were more confident with their initial answers, regardless of accuracy, were less likely to revise their responses when persuaded by AI (than by human). Taken together, these insights are vital for marketing academics and practitioners aiming to optimize AI integration in service-oriented industries while maintaining consumer trust and engagement.",
    "session_title": "AI, Work & Organizations",
    "date": "Oct 2"
  },
  {
    "title": "Why the Best Machine May Not Be the Best: Incentivizing Human\u2013Machine Collaboration",
    "authors": [
      {
        "name": "Xiaotong Guan",
        "affiliation": "The University of Texas at Dallas"
      },
      {
        "name": "Anyan Qi",
        "affiliation": "The University of Texas at Dallas"
      },
      {
        "name": "Shouqiang Wang",
        "affiliation": "The University of Texas at Dallas"
      }
    ],
    "presenter_first_name": "Xiaotong",
    "presenter_last_name": "Guan",
    "presenter_email": "xiaotong.guan@utdallas.edu",
    "presenter_affiliation": "The University of Texas at Dallas",
    "presentation_room": "Pfahl 140",
    "time": "10:00 - 11:00",
    "reported_discipline": "Operations Management",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_4AExg0cbLwqNGgu",
    "abstract": "As AI reshapes our world, robust human\u2011machine collaboration is paramount to modernizing crucial operational decisions such as demand forecasts. In this paper, we study how the machine should be designed to incentivize human contributions. In our setting, a principal designs a forecasting algorithm by specifying its precision and delegates the final forecasting task to an agent. After receiving the machine\u2019s forecast, the agent decides whether to acquire an additional private signal before making the final forecast. We find that the principal may optimally reduce the machine\u2019s precision below its technological limit\u2014even when enhancing such precision within that limit is costless. Essentially, the machine\u2019s precision not only serves a functional role but also acts as an incentive instrument to motivate the agent to gather additional information. Interestingly, the agent may over\u2011acquire more information, resulting in possibly higher forecasting accuracy, than what would be optimal if the forecasting task were not delegated. On an optimistic note, we find that humans and machines are complementary: the better the information the agent can acquire, the more precise the machine should be.",
    "session_title": "AI & Scaling",
    "date": "Oct 3"
  },
  {
    "title": "Why Students Reject AI for Human Counselors in College Applications: A Field Experiment",
    "authors": [
      {
        "name": "Hemanshu Das",
        "affiliation": "Yale School of Management"
      },
      {
        "name": "Sofoklis Goulas",
        "affiliation": "Yale School of Management"
      },
      {
        "name": "Faidra Monachou",
        "affiliation": "Yale School of Management"
      }
    ],
    "presenter_first_name": "Hemanshu",
    "presenter_last_name": "Das",
    "presenter_email": "hemanshu.das@yale.edu",
    "presenter_affiliation": "Yale School of Management",
    "presentation_room": "Pfahl 140",
    "time": "1:00 - 2:40",
    "reported_discipline": "Operations Research",
    "cleaned_discipline": "Logistics, Operations, and Supply Chain Management",
    "best_phd_finalist": true,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_6ozQog8eYD2U01i",
    "abstract": "The adoption of AI technology in education has fueled a surge of ambitious initiatives, driven by its transformative potential. Yet, despite large investments, adoption of AI tools remains slow \u2014 often hindered by low trust and concerns about fairness and effectiveness. While algorithm aversion is well-documented, its role in education, especially among adolescents making high- stakes decisions, remains underexplored. This study investigates how students respond to college application recommendations when attributed to AI-based algorithms versus human counselors. We ran a lab-in-the-field , survey-based experiment among 2082 students across 14 public high schools in Greece\u2014a country with a centralized college admissions system similar to 40% of countries globally. Students evaluated identical recommendations from either algorithms or human counselors across three hypothetical scenarios: Heart (interests vs. prestige), Geography (location vs. prestige), and Pragmatism (admissions chances vs. prestige). These scenarios represent varying degrees of objectivity in the recommendation basis, from most subjective ( Heart ) to most objective ( Pragmatism ). Students displayed greater aversion to algorithmic recommendations compared to those from human counselors when the recommendation basis was more objective. The Pragmatism scenario revealed the largest adoption gap \u2014 4.7 percentage points favoring human counselors. In contrast, algorithmic and human recommendations performed similarly in the subjective Heart scenario. Recommendation adoption rates exhibit substantial heterogeneity by various student and school characteristics. Female students were less likely to adopt recommendations in subjective scenarios but more likely in the objective one. The adoption gap was largest in the Pragmatism scenario for both genders, and statistically significant for female students. Student performance also shaped preferences; high-achieving students showed no significant adoption gap in Pragmatism , whereas low-achieving students strongly favored human counselors (8.0-point gap). Rural students signifi- cantly preferred human counselors in the objective scenario (9.3-point gap), while urban students exhibited no clear preference. Students\u2019 compliance with the social norm of \u201cprestige-chasing\u201d (pursuing the most prestigious program regardless of interest) was associated with increased aversion to algorithmic advice in objective scenarios. We also collected and analyzed students\u2019 free-text responses. The responses suggest that students wish to seek guidance and information about alternative study options from human counselors but will turn to algorithms for recommendations based on grades and admissions chances. Finally, using an optimization approach, we demonstrate how a planner can design a hybrid, human-in-the-loop assignment policy while navigating the heterogeneity in adoption. We provide a framework to optimally prioritize the assignment of human versus algorithmic recommenders, under varying social preferences and limited capacity of human counselors. We find that a targeting policy relying on a few readily available student and school features can approximate the first-best, personalized targeting policy effectively. Our findings underscore the importance of a user-first approach in building AI systems that align better with users\u2019 needs and resonate with the target audience before real-world implementation. Our results can serve as a blueprint for applications beyond education. Our work offers actionable insights for integrating AI into high-stakes service settings where trust and human judgment re- main critical. This study speaks directly to the goal of AI in Business conference by evaluating how human-AI hybrid system can enhance decision-making in education. We combine behavioral experimentation with optimization to design responsible, equitable, and human-centered AI de- ployments\u2014aligning closely with the conference\u2019s focus on oversight and accountability.",
    "session_title": "Best Presentation Competition \u2013 Doctoral Students",
    "date": "Oct 2"
  },
  {
    "title": "Work Design for AI Agents: Tool and Socio-Technical System Design for AI Performance",
    "authors": [
      {
        "name": "Cory Eisenhard",
        "affiliation": "Michigan State University"
      },
      {
        "name": "Frederick P. Morgeson",
        "affiliation": "Michigan State University"
      },
      {
        "name": "Dhruv K. Toprani",
        "affiliation": "Michigan State University"
      }
    ],
    "presenter_first_name": "Cory",
    "presenter_last_name": "Eisenhard",
    "presenter_email": "coryeisenhard@gmail.com",
    "presenter_affiliation": "Michigan State University",
    "presentation_room": "Pfahl 202",
    "time": "1:00 - 2:40",
    "reported_discipline": "Management",
    "cleaned_discipline": "Management and Human Resources",
    "best_phd_finalist": null,
    "best_junior_faculty_finalist": null,
    "pdf_id": "R_3bVKDMdCo3oemi5",
    "abstract": "Advances in large-scale foundation models have enabled artificial intelligence (AI) to move beyond automating routine work. Today, AI is capable of completing complex tasks, such as interpreting medical charts, inventing mathematical concepts, and generating novel ideas. Yet, even the most capable systems remain surprisingly fragile when executing end-to-end tasks in real-world work settings. This reflects not an immutable limitation of current AI but a mismatch between work contexts architected for human workers and the allowances required by disembodied AI agents. To overcome this challenge, we build on Kurzweil\u2019s (2005, 2024) theoretical proposition in the Law of Accelerating Returns (LAR), which suggests that each generation of information processing technology recursively compounds the capabilities of its predecessors through diverse mechanisms. Specifically, we develop a theory of AI work design that explains how tools and socio-technical systems can multiply performance gains delivered by hardware improvements in isolation (Moore, 1965, 1975). We make two primary contributions. First, we integrate LAR with work design theory to articulate how accelerating improvements across hardware, software, and organizational context jointly shape the performance of AI agents (Kong et al., 2024; Ruan et al., 2023). This synthesis emphasizes seven work characteristics associated with AI agents\u2019 work that are potential performance bottlenecks: AI tool design, task variety, feedback from the job, job complexity, information processing, problem solving, and interdependence (Morgeson & Humphrey, 2006; Parker et al., 2017). AI tool design, defined as the degree to which specific tools are available for AI agents to complete the work, is accorded central status because seemingly trivial tasks can be challenging if AI agents lack the right tools for the job. For example, pressing a button on analog equipment can be an insurmountable challenge for an otherwise superintelligent AI agent if it lacks tools that can alert a human worker or operate a robotic hand (Faulkner & Runde, 2019). Second, we instantiate our theoretical framework in an agent-based simulation that embeds a multi-modal Large Language Model as an AI agent operating within a real-time game environment (a maze). We systematically test the AI agent by varying AI tool design and other work characteristics, and record four objective performance indicators: number of game actions, API calls, API tokens consumed, and tasks completed in clock time. By comparing conditions in which the agent works unaided versus scenarios in which it can invoke task-specific tools, we estimate the interaction effects predicted by LAR. This research will contribute to the inaugural AI in Business Conference by offering insights on the changing capabilities of human workers and AI agents, and their complementary roles in the future of work. We extend work design theory to consider the nature of work for AI agents and will present a tractable modeling platform for future empirical tests of the effect of specific AI tools and socio-technical systems on AI agent work performance. Our theory and results open opportunities for multidisciplinary collaboration as performance optimizations for AI agents become increasingly critical to the design of work across business functions.",
    "session_title": "AI, Work & Organizations",
    "date": "Oct 2"
  }
]